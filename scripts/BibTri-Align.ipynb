{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18344f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 66 complete triplets.\n",
      "Saved combined CSV: Bible_Align\\data\\BTA_aligned_all.csv\n",
      "Saved combined JSONL: Bible_Align\\data\\BTA_aligned_all.jsonl\n",
      "Coverage summary saved: Bible_Align\\data\\coverage_by_file.csv\n",
      "        base book_inferred  total_rows  complete_rows  missing_rows  pct_complete  su_parenthetical_only_rows\n",
      "  1_korintus    1 Korintus          58             58             0         100.0                           0\n",
      "    1_petrus      1 Petrus          25             25             0         100.0                           0\n",
      "1_tesalonika  1 Tesalonika          28             28             0         100.0                           0\n",
      "   1_yohanes     1 Yohanes          29             29             0         100.0                           0\n",
      " 2_raja-raja   2 Raja-Raja          44             44             0         100.0                           0\n",
      "    2_samuel      2 Samuel          51             51             0         100.0                           0\n",
      "  2_tawarikh    2 Tawarikh          42             42             0         100.0                           0\n",
      "2_tesalonika  2 Tesalonika          18             18             0         100.0                           0\n",
      "  2_timotius    2 Timotius          26             26             0         100.0                           0\n",
      "   2_yohanes     2 Yohanes          13             13             0         100.0                           0\n",
      "   3_yohanes     3 Yohanes          14             14             0         100.0                           0\n",
      "        amos          Amos          27             27             0         100.0                           0\n",
      "       amsal         Amsal          36             36             0         100.0                           0\n",
      "        ayub          Ayub          41             41             0         100.0                           0\n",
      "      daniel        Daniel          49             49             0         100.0                           0\n",
      "Per-book CSVs written in C:\\Users\\jtkacer01\\00PlayGround\\00 BIBLE MT\\Bible_Align\\data\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# BTA: Folder-based Triple Aligner (EN–ID–SU)\n",
    "# With Sundanese parenthetical-only flagging baked in\n",
    "# ================================\n",
    "from pathlib import Path\n",
    "import re, html, json\n",
    "import pandas as pd\n",
    "\n",
    "# ------------\n",
    "# CONFIG\n",
    "# ------------\n",
    "DIR_EN = Path(\"Bible/inggris/\")     # folder with *_inggris.txt\n",
    "DIR_ID = Path(\"Bible/indonesia/\")  # folder with *_indonesia.txt\n",
    "DIR_SU = Path(\"Bible/sunda/\")   # folder with *_sunda.txt\n",
    "\n",
    "SUFFIX_EN = \"_inggris\"\n",
    "SUFFIX_ID = \"_indonesia\"\n",
    "SUFFIX_SU = \"_sunda\"\n",
    "\n",
    "USE_FILENAME_AS_BOOK = True\n",
    "BOOK_NAME_NORMALIZATION = {}\n",
    "\n",
    "# OUTPUT to Bible_Align/data  # <<< NEW\n",
    "OUT_DIR = Path(\"Bible_Align/data\")\n",
    "SPLIT_BY_BOOK = True\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------\n",
    "# Parsers & helpers\n",
    "# ------------\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "CHAPTER_HDR_RE = re.compile(r\"^\\s*([0-9A-Za-z .’'\\-]+?)\\s+(\\d+)\\s*$\")\n",
    "VERSE_LINE_RE   = re.compile(r\"^\\s*(\\d+)\\s+(.*\\S)\\s*$\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = html.unescape(s).replace(\"\\u00A0\", \" \")\n",
    "    return WHITESPACE_RE.sub(\" \", s).strip()\n",
    "\n",
    "def normalize_book(name: str) -> str:\n",
    "    name = clean_text(name)\n",
    "    return BOOK_NAME_NORMALIZATION.get(name, name)\n",
    "\n",
    "def base_from_name(fname: str, suffix: str) -> str:\n",
    "    if fname.lower().endswith(\".txt\"): fname = fname[:-4]\n",
    "    return fname[:-len(suffix)] if fname.endswith(suffix) else fname\n",
    "\n",
    "def base_to_book(base: str) -> str:\n",
    "    book = base.replace(\"_\", \" \").strip()\n",
    "    if book and book[0].isdigit():\n",
    "        parts = book.split(\" \", 1)\n",
    "        book = parts[0] + \" \" + (parts[1].title() if len(parts) == 2 else \"\")\n",
    "    else:\n",
    "        book = book.title()\n",
    "    return normalize_book(book)\n",
    "\n",
    "def parse_bible_plaintext(path: Path, lang_code: str, single_book: str | None = None):\n",
    "    out, book, chapter = [], (normalize_book(single_book) if single_book else None), None\n",
    "    if not path.exists(): return out\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for raw in f:\n",
    "            line = clean_text(raw)\n",
    "            if not line: continue\n",
    "            m_hdr = CHAPTER_HDR_RE.match(line)\n",
    "            if m_hdr and single_book is None:\n",
    "                book, chapter = normalize_book(m_hdr.group(1)), int(m_hdr.group(2)); continue\n",
    "            m_v = VERSE_LINE_RE.match(line)\n",
    "            if m_v:\n",
    "                vnum, vtxt = int(m_v.group(1)), clean_text(m_v.group(2))\n",
    "                if book is None and single_book: book = normalize_book(single_book)\n",
    "                if book is None: book = \"UNKNOWN_BOOK\"\n",
    "                if chapter is None: chapter = 1\n",
    "                out.append({\"book\": book, \"chapter\": int(chapter), \"verse\": vnum, \"text\": vtxt, \"lang\": lang_code})\n",
    "            elif out and out[-1][\"lang\"] == lang_code:\n",
    "                out[-1][\"text\"] = clean_text(out[-1][\"text\"] + \" \" + line)\n",
    "    return out\n",
    "\n",
    "def to_frame(recs): return pd.DataFrame.from_records(recs, columns=[\"book\",\"chapter\",\"verse\",\"lang\",\"text\"])\n",
    "\n",
    "def align_three(df_en, df_id, df_su):\n",
    "    wide_en = df_en.pivot_table(index=[\"book\",\"chapter\",\"verse\"], values=\"text\", aggfunc=\"first\")\n",
    "    wide_id = df_id.pivot_table(index=[\"book\",\"chapter\",\"verse\"], values=\"text\", aggfunc=\"first\")\n",
    "    wide_su = df_su.pivot_table(index=[\"book\",\"chapter\",\"verse\"], values=\"text\", aggfunc=\"first\")\n",
    "    aligned = wide_en.join(wide_id, how=\"outer\", lsuffix=\"_en\").join(wide_su, how=\"outer\", rsuffix=\"_su\")\n",
    "    rename_map = {}\n",
    "    for c in aligned.columns:\n",
    "        lc = c.lower()\n",
    "        rename_map[c] = \"en_text\" if lc.endswith(\"_en\") or lc==\"text_en\" else \\\n",
    "                        \"su_text\" if lc.endswith(\"_su\") or lc==\"text_su\" else \\\n",
    "                        \"id_text\"\n",
    "    aligned = aligned.rename(columns=rename_map).reset_index().sort_values([\"book\",\"chapter\",\"verse\"])\n",
    "    for col in [\"en_text\",\"id_text\",\"su_text\"]:\n",
    "        if col not in aligned.columns: aligned[col] = pd.NA\n",
    "    return aligned[[\"book\",\"chapter\",\"verse\",\"en_text\",\"id_text\",\"su_text\"]]\n",
    "\n",
    "def mismatch_report(aligned: pd.DataFrame) -> pd.DataFrame:\n",
    "    return aligned[aligned[[\"en_text\",\"id_text\",\"su_text\"]].isna().any(axis=1)]\n",
    "\n",
    "# <<< NEW: parenthetical-only detector for Sundanese\n",
    "PAREN_REF_RE = re.compile(r\"^\\(\\s*\\d+\\s*:\\s*\\d+(?:\\s*[-–]\\s*\\d+)?\\s*\\)$\")\n",
    "\n",
    "def apply_sundanese_parenthetical_flag(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    su = df[\"su_text\"].fillna(\"\")\n",
    "    mask = su.str.match(PAREN_REF_RE)\n",
    "    df[\"su_missing_parenthetical\"] = mask\n",
    "    df.loc[mask, \"su_text\"] = pd.NA\n",
    "    return df\n",
    "\n",
    "# ------------\n",
    "# Discover triplets and process\n",
    "# ------------\n",
    "def discover_triplets(dir_en: Path, dir_id: Path, dir_su: Path):\n",
    "    en_files = {base_from_name(p.name, SUFFIX_EN): p for p in dir_en.glob(\"*.txt\")}\n",
    "    id_files = {base_from_name(p.name, SUFFIX_ID): p for p in dir_id.glob(\"*.txt\")}\n",
    "    su_files = {base_from_name(p.name, SUFFIX_SU): p for p in dir_su.glob(\"*.txt\")}\n",
    "    bases_all = set(en_files) | set(id_files) | set(su_files)\n",
    "    triplets, missing = [], []\n",
    "    for base in sorted(bases_all):\n",
    "        p_en, p_id, p_su = en_files.get(base), id_files.get(base), su_files.get(base)\n",
    "        (triplets.append((base, p_en, p_id, p_su)) if (p_en and p_id and p_su)\n",
    "         else missing.append({\"base\": base, \"has_en\": bool(p_en), \"has_id\": bool(p_id), \"has_su\": bool(p_su)}))\n",
    "    return triplets, missing\n",
    "\n",
    "triplets, missing = discover_triplets(DIR_EN, DIR_ID, DIR_SU)\n",
    "print(f\"Discovered {len(triplets)} complete triplets.\")\n",
    "if missing:\n",
    "    import pandas as _pd\n",
    "    print(f\"{len(missing)} bases missing at least one language. First 10:\")\n",
    "    print(_pd.DataFrame(missing).head(10).to_string(index=False))\n",
    "\n",
    "all_aligned, coverage_rows = [], []\n",
    "\n",
    "for base, p_en, p_id, p_su in triplets:\n",
    "    single_book = base_to_book(base) if USE_FILENAME_AS_BOOK else None\n",
    "    df_en = to_frame(parse_bible_plaintext(p_en, \"en\", single_book))\n",
    "    df_id = to_frame(parse_bible_plaintext(p_id, \"id\", single_book))\n",
    "    df_su = to_frame(parse_bible_plaintext(p_su, \"su\", single_book))\n",
    "    aligned = align_three(df_en, df_id, df_su)\n",
    "\n",
    "    # Apply Sundanese parenthetical-only rule  # <<< NEW\n",
    "    aligned = apply_sundanese_parenthetical_flag(aligned)\n",
    "\n",
    "    aligned[\"source_base\"] = base\n",
    "    all_aligned.append(aligned)\n",
    "\n",
    "    total = aligned.shape[0]\n",
    "    miss = mismatch_report(aligned)\n",
    "    coverage_rows.append({\n",
    "        \"base\": base,\n",
    "        \"book_inferred\": single_book if single_book else \"(headers)\",\n",
    "        \"total_rows\": total,\n",
    "        \"complete_rows\": total - len(miss),\n",
    "        \"missing_rows\": len(miss),\n",
    "        \"pct_complete\": 0 if total == 0 else round(100.0*(total - len(miss))/total, 2),\n",
    "        # how many rows were parenthetical-only in SU  # <<< NEW\n",
    "        \"su_parenthetical_only_rows\": int(aligned[\"su_missing_parenthetical\"].sum())\n",
    "    })\n",
    "\n",
    "# ------------\n",
    "# Concatenate and save\n",
    "# ------------\n",
    "big = pd.concat(all_aligned, ignore_index=True) if all_aligned else \\\n",
    "      pd.DataFrame(columns=[\"book\",\"chapter\",\"verse\",\"en_text\",\"id_text\",\"su_text\",\"su_missing_parenthetical\",\"source_base\"])\n",
    "\n",
    "combined_csv  = OUT_DIR / \"BTA_aligned_all.csv\"\n",
    "combined_json = OUT_DIR / \"BTA_aligned_all.jsonl\"\n",
    "big.to_csv(combined_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "with combined_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for _, r in big.iterrows():\n",
    "        f.write(json.dumps({\n",
    "            \"book\": r.get(\"book\"),\n",
    "            \"chapter\": int(r[\"chapter\"]) if pd.notna(r.get(\"chapter\")) else None,\n",
    "            \"verse\": int(r[\"verse\"]) if pd.notna(r.get(\"verse\")) else None,\n",
    "            \"en_text\": None if pd.isna(r.get(\"en_text\")) else r.get(\"en_text\"),\n",
    "            \"id_text\": None if pd.isna(r.get(\"id_text\")) else r.get(\"id_text\"),\n",
    "            \"su_text\": None if pd.isna(r.get(\"su_text\")) else r.get(\"su_text\"),\n",
    "            \"su_missing_parenthetical\": bool(r.get(\"su_missing_parenthetical\")),\n",
    "            \"source_base\": r.get(\"source_base\")\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved combined CSV: {combined_csv}\")\n",
    "print(f\"Saved combined JSONL: {combined_json}\")\n",
    "\n",
    "# Coverage report (includes su_parenthetical_only_rows)  # <<< NEW\n",
    "coverage = pd.DataFrame(coverage_rows).sort_values([\"pct_complete\",\"base\"], ascending=[False, True])\n",
    "coverage_csv = OUT_DIR / \"coverage_by_file.csv\"\n",
    "coverage.to_csv(coverage_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"Coverage summary saved: {coverage_csv}\")\n",
    "print(coverage.head(15).to_string(index=False))\n",
    "\n",
    "# Optional per-book splits\n",
    "if SPLIT_BY_BOOK and not big.empty:\n",
    "    for book, grp in big.groupby(\"book\"):\n",
    "        safe_book = re.sub(r\"[^0-9A-Za-z _.-]\", \"_\", book or \"UNKNOWN\")\n",
    "        (OUT_DIR / f\"BTA_{safe_book}.csv\").write_text(grp.sort_values([\"chapter\",\"verse\"]).to_csv(index=False), encoding=\"utf-8\")\n",
    "    print(f\"Per-book CSVs written in {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2101b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 4875 sentence rows to:\n",
      " - out_preprocess\\tri_sentences.csv\n",
      " - out_preprocess\\tri_sentences.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing & Segmentation (robust to your columns)\n",
    "# Accepts CSV or JSONL with columns like:\n",
    "#   book, chapter, verse, su_text, id_text, en_text, ... (your file)\n",
    "# or the simpler schema:\n",
    "#   verse_id, su, id, en\n",
    "\n",
    "import re, csv, json, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# ====== CONFIG ======\n",
    "INPUT_PATH = Path(\"Bible_Align/data/BTA_aligned_all.csv\")   # or .jsonl\n",
    "OUT_DIR = Path(\"./out_preprocess\")\n",
    "WRITE_TOKENS = True\n",
    "LOWERCASE = False\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====== Normalization ======\n",
    "PUNCT_MAP = {\"“\":\"\\\"\", \"”\":\"\\\"\", \"‘\":\"'\", \"’\":\"'\", \"–\":\"-\", \"—\":\"-\", \"…\":\"...\", \"«\":\"\\\"\", \"»\":\"\\\"\"}\n",
    "TRANS = str.maketrans(PUNCT_MAP)\n",
    "\n",
    "def normalize_text(text: str, lowercase: bool = LOWERCASE) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", text).translate(TRANS)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "    return s\n",
    "\n",
    "# ====== Sentence segmentation ======\n",
    "ID_ABBR = {\"dr\",\"sdr\",\"sdri\",\"dll\",\"dsb\",\"sbb\",\"no\",\"hlm\"}\n",
    "EN_ABBR = {\"mr\",\"mrs\",\"ms\",\"dr\",\"prof\",\"vs\",\"etc\",\"e.g\",\"i.e\"}\n",
    "SU_ABBR = set()\n",
    "ABBR = ID_ABBR | EN_ABBR | SU_ABBR\n",
    "ABBR_PAT = re.compile(r\"\\b(?:(?:\" + \"|\".join(sorted(ABBR)) + r\")\\.)$\", re.IGNORECASE) if ABBR else None\n",
    "\n",
    "def sent_split(lang: str, text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r\"(?<=[\\.!\\?])\\s+(?=[A-Z0-9\\\"'“‘(])\", text)\n",
    "    merged = []\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if not p: \n",
    "            continue\n",
    "        if merged:\n",
    "            prev = merged[-1]\n",
    "            last_tok = prev.split()[-1] if prev.split() else \"\"\n",
    "            if last_tok.endswith(\".\") and ABBR_PAT and ABBR_PAT.search(last_tok):\n",
    "                merged[-1] = prev + \" \" + p\n",
    "                continue\n",
    "        merged.append(p)\n",
    "    return merged\n",
    "\n",
    "# ====== Tokenization ======\n",
    "TOKEN_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ0-9']+|[^\\w\\s]\", re.UNICODE)\n",
    "def tokenize(text: str):\n",
    "    return [t for t in TOKEN_RE.findall(text or \"\") if t.strip()]\n",
    "\n",
    "# ====== Schema utilities ======\n",
    "def build_verse_id(book: str, chapter, verse) -> str:\n",
    "    # build a readable verse_id like \"1 Korintus:1:1\" or \"Genesis:1:1\"\n",
    "    b = normalize_text(book, lowercase=False)\n",
    "    # keep spaces in book to stay human-friendly\n",
    "    return f\"{b}:{str(chapter).strip()}:{str(verse).strip()}\"\n",
    "\n",
    "def unify_row(r: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Map an arbitrary source row to a unified record with keys:\n",
    "    verse_id, su, id, en\n",
    "    \"\"\"\n",
    "    keys = {k.lower(): k for k in r.keys()}\n",
    "\n",
    "    # text fields (prefer *_text if present)\n",
    "    su = r.get(keys.get(\"su_text\",\"\"), \"\") or r.get(keys.get(\"su\",\"\"), \"\")\n",
    "    id_ = r.get(keys.get(\"id_text\",\"\"), \"\") or r.get(keys.get(\"id\",\"\"), \"\")\n",
    "    en = r.get(keys.get(\"en_text\",\"\"), \"\") or r.get(keys.get(\"en\",\"\"), \"\")\n",
    "\n",
    "    # verse_id direct or composed from book/chapter/verse\n",
    "    if \"verse_id\" in keys:\n",
    "        verse_id = r[keys[\"verse_id\"]]\n",
    "    else:\n",
    "        # Need book, chapter, verse\n",
    "        book = r.get(keys.get(\"book\",\"\"), \"\")\n",
    "        chapter = r.get(keys.get(\"chapter\",\"\"), \"\")\n",
    "        verse = r.get(keys.get(\"verse\",\"\"), \"\")\n",
    "        if not (book and chapter and verse):\n",
    "            raise ValueError(\"Cannot build verse_id: need columns 'book', 'chapter', 'verse' or 'verse_id'.\")\n",
    "        verse_id = build_verse_id(book, chapter, verse)\n",
    "\n",
    "    return {\n",
    "        \"verse_id\": normalize_text(verse_id, lowercase=False),\n",
    "        \"su\": normalize_text(su),\n",
    "        \"id\": normalize_text(id_),\n",
    "        \"en\": normalize_text(en),\n",
    "    }\n",
    "\n",
    "def read_input(path: Path):\n",
    "    rows = []\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            rd = csv.DictReader(f)\n",
    "            for r in rd:\n",
    "                rows.append(unify_row(r))\n",
    "    elif path.suffix.lower() in (\".jsonl\", \".json\"):\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip(): \n",
    "                    continue\n",
    "                obj = json.loads(line)\n",
    "                rows.append(unify_row(obj))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input format. Use .csv or .jsonl\")\n",
    "    return rows\n",
    "\n",
    "# ====== Main processing ======\n",
    "def process(input_path: Path, out_dir: Path):\n",
    "    src_rows = read_input(input_path)\n",
    "\n",
    "    out_rows = []\n",
    "    for r in src_rows:\n",
    "        verse_id = r[\"verse_id\"]\n",
    "        su_text, id_text, en_text = r[\"su\"], r[\"id\"], r[\"en\"]\n",
    "\n",
    "        su_sents = sent_split(\"su\", su_text)\n",
    "        id_sents = sent_split(\"id\", id_text)\n",
    "        en_sents = sent_split(\"en\", en_text)\n",
    "\n",
    "        max_len = max(len(su_sents), len(id_sents), len(en_sents)) or 1\n",
    "        for i in range(max_len):\n",
    "            su_i = su_sents[i] if i < len(su_sents) else \"\"\n",
    "            id_i = id_sents[i] if i < len(id_sents) else \"\"\n",
    "            en_i = en_sents[i] if i < len(en_sents) else \"\"\n",
    "            if not (su_i or id_i or en_i):\n",
    "                continue\n",
    "            sent_idx = i + 1\n",
    "            sent_id = f\"{verse_id}#{sent_idx}\"\n",
    "            row = {\n",
    "                \"verse_id\": verse_id,\n",
    "                \"sent_idx\": sent_idx,\n",
    "                \"sent_id\": sent_id,\n",
    "                \"su_sentence\": su_i,\n",
    "                \"id_sentence\": id_i,\n",
    "                \"en_sentence\": en_i,\n",
    "            }\n",
    "            if WRITE_TOKENS:\n",
    "                row[\"su_tokens\"] = \" \".join(tokenize(su_i))\n",
    "                row[\"id_tokens\"] = \" \".join(tokenize(id_i))\n",
    "                row[\"en_tokens\"] = \" \".join(tokenize(en_i))\n",
    "            out_rows.append(row)\n",
    "\n",
    "    out_csv = out_dir / \"tri_sentences.csv\"\n",
    "    if out_rows:\n",
    "        with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(out_rows[0].keys()))\n",
    "            writer.writeheader()\n",
    "            writer.writerows(out_rows)\n",
    "\n",
    "    out_jsonl = out_dir / \"tri_sentences.jsonl\"\n",
    "    with out_jsonl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for row in out_rows:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out_rows)} sentence rows to:\")\n",
    "    print(f\" - {out_csv}\")\n",
    "    print(f\" - {out_jsonl}\")\n",
    "\n",
    "# ====== RUN ======\n",
    "process(INPUT_PATH, OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1228473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote bitext:\n",
      " - out_align\\su_id.bitext (4583 lines)\n",
      " - out_align\\su_en.bitext (4803 lines)\n",
      "\n",
      "[WARN] fast_align not found on PATH. Skipping execution.\n",
      "You can run these commands locally once fast_align is installed:\n",
      "\n",
      "  fast_align -i out_align\\su_id.bitext -d -o -v > out_align\\su_id.fwd.align\n",
      "  fast_align -i out_align\\su_id.bitext -d -o -v -r > out_align\\su_id.rev.align\n",
      "  (Then symmetrize with grow-diag-final using the Python code in this cell.)\n",
      "\n",
      "[WARN] fast_align not found on PATH. Skipping execution.\n",
      "You can run these commands locally once fast_align is installed:\n",
      "\n",
      "  fast_align -i out_align\\su_en.bitext -d -o -v > out_align\\su_en.fwd.align\n",
      "  fast_align -i out_align\\su_en.bitext -d -o -v -r > out_align\\su_en.rev.align\n",
      "  (Then symmetrize with grow-diag-final using the Python code in this cell.)\n",
      "\n",
      "[INFO] fast_align not executed. Bitext files are ready; run fast_align manually,\n",
      "then re-run the bottom block of this cell to attach alignments back into the CSV.\n",
      "Bitext files:\n",
      " - out_align\\su_id.bitext\n",
      " - out_align\\su_en.bitext\n"
     ]
    }
   ],
   "source": [
    "import csv, os, shutil, subprocess, sys, re\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# ====== CONFIG ======\n",
    "IN_SENTENCES = Path(\"./out_preprocess/tri_sentences.csv\")  # from your preprocessing step\n",
    "WORK_DIR     = Path(\"./out_align\")\n",
    "LANG_SRC     = \"su\"  # source for both pairs\n",
    "LANG_TGT_1   = \"id\"  # pair 1: su-id\n",
    "LANG_TGT_2   = \"en\"  # pair 2: su-en\n",
    "\n",
    "# fast_align binaries (change if installed elsewhere)\n",
    "FAST_ALIGN_BIN = shutil.which(\"fast_align\")\n",
    "ATOOLS_BIN     = shutil.which(\"atools\")   # optional; we also have a Python symmetrizer below\n",
    "\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====== Helpers ======\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def write_bitext(in_csv: Path, out_bitext_path: Path, src_col: str, tgt_col: str) -> int:\n",
    "    n = 0\n",
    "    with in_csv.open(\"r\", encoding=\"utf-8\") as fi, out_bitext_path.open(\"w\", encoding=\"utf-8\") as fo:\n",
    "        rd = csv.DictReader(fi)\n",
    "        for row in rd:\n",
    "            src = normalize_spaces(row.get(src_col, \"\"))\n",
    "            tgt = normalize_spaces(row.get(tgt_col, \"\"))\n",
    "            # skip empty pairs\n",
    "            if not src and not tgt:\n",
    "                continue\n",
    "            fo.write(f\"{src} ||| {tgt}\\n\")\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "def run(cmd: List[str]) -> subprocess.CompletedProcess:\n",
    "    return subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)\n",
    "\n",
    "def parse_alignment_line(line: str) -> Set[Tuple[int,int]]:\n",
    "    # \"0-0 1-2 ...\"  -> {(0,0), (1,2), ...}\n",
    "    pairs = set()\n",
    "    for tok in line.strip().split():\n",
    "        if \"-\" in tok:\n",
    "            a,b = tok.split(\"-\",1)\n",
    "            try:\n",
    "                pairs.add((int(a), int(b)))\n",
    "            except:\n",
    "                pass\n",
    "    return pairs\n",
    "\n",
    "def format_alignment_line(points: Set[Tuple[int,int]]) -> str:\n",
    "    # sorted for stability\n",
    "    return \" \".join([f\"{i}-{j}\" for (i,j) in sorted(points)])\n",
    "\n",
    "def grow_diag_final(forward: Set[Tuple[int,int]], reverse: Set[Tuple[int,int]],\n",
    "                    src_len: int, tgt_len: int) -> Set[Tuple[int,int]]:\n",
    "    \"\"\"\n",
    "    Standard grow-diag-final symmetrization (Och & Ney 2003-ish variant).\n",
    "    - start with intersection\n",
    "    - grow-diag: iteratively add neighbors present in union if they link an unaligned word\n",
    "    - final: add remaining points from forward/reverse that link unaligned words\n",
    "    \"\"\"\n",
    "    inter = forward & reverse\n",
    "    union = forward | reverse\n",
    "    alignment = set(inter)\n",
    "\n",
    "    def aligned_src(a): return {i for (i,_) in a}\n",
    "    def aligned_tgt(a): return {j for (_,j) in a}\n",
    "\n",
    "    def neighbors(i,j):\n",
    "        # 8-neighborhood\n",
    "        for di in (-1,0,1):\n",
    "            for dj in (-1,0,1):\n",
    "                if di == 0 and dj == 0: \n",
    "                    continue\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < src_len and 0 <= nj < tgt_len:\n",
    "                    yield (ni, nj)\n",
    "\n",
    "    added = True\n",
    "    while added:\n",
    "        added = False\n",
    "        A_src = aligned_src(alignment)\n",
    "        A_tgt = aligned_tgt(alignment)\n",
    "        cand = set()\n",
    "        for (i,j) in alignment:\n",
    "            for nb in neighbors(i,j):\n",
    "                if nb in union and nb not in alignment:\n",
    "                    si, tj = nb\n",
    "                    if si not in A_src or tj not in A_tgt:\n",
    "                        cand.add(nb)\n",
    "        if cand:\n",
    "            alignment |= cand\n",
    "            added = True\n",
    "\n",
    "    # final step: add remaining points from forward/reverse that link unaligned words\n",
    "    A_src = aligned_src(alignment)\n",
    "    A_tgt = aligned_tgt(alignment)\n",
    "    for (i,j) in forward:\n",
    "        if (i not in A_src) or (j not in A_tgt):\n",
    "            alignment.add((i,j))\n",
    "    A_src = aligned_src(alignment)\n",
    "    A_tgt = aligned_tgt(alignment)\n",
    "    for (i,j) in reverse:\n",
    "        if (i not in A_src) or (j not in A_tgt):\n",
    "            alignment.add((i,j))\n",
    "\n",
    "    return alignment\n",
    "\n",
    "def count_tokens_pair(line: str) -> Tuple[int,int]:\n",
    "    # from \"src ||| tgt\" count tokens by whitespace\n",
    "    src, tgt = line.split(\"|||\")\n",
    "    s = len(src.strip().split())\n",
    "    t = len(tgt.strip().split())\n",
    "    return s, t\n",
    "\n",
    "# ====== Prepare bitext for both pairs ======\n",
    "su_id_bitext = WORK_DIR / f\"{LANG_SRC}_{LANG_TGT_1}.bitext\"\n",
    "su_en_bitext = WORK_DIR / f\"{LANG_SRC}_{LANG_TGT_2}.bitext\"\n",
    "\n",
    "n1 = write_bitext(IN_SENTENCES, su_id_bitext, \"su_sentence\", \"id_sentence\")\n",
    "n2 = write_bitext(IN_SENTENCES, su_en_bitext, \"su_sentence\", \"en_sentence\")\n",
    "print(f\"Wrote bitext:\\n - {su_id_bitext} ({n1} lines)\\n - {su_en_bitext} ({n2} lines)\")\n",
    "\n",
    "# ====== Align a single bitext with fast_align, returning (fwd, rev, symm) paths ======\n",
    "def align_pair(bitext_path: Path, pair_tag: str):\n",
    "    fwd_out = WORK_DIR / f\"{pair_tag}.fwd.align\"\n",
    "    rev_out = WORK_DIR / f\"{pair_tag}.rev.align\"\n",
    "    sym_out = WORK_DIR / f\"{pair_tag}.sym.gdf\"\n",
    "\n",
    "    # Detect fast_align\n",
    "    if FAST_ALIGN_BIN is None:\n",
    "        print(\"\\n[WARN] fast_align not found on PATH. Skipping execution.\")\n",
    "        print(\"You can run these commands locally once fast_align is installed:\\n\")\n",
    "        print(f\"  {FAST_ALIGN_BIN or 'fast_align'} -i {bitext_path} -d -o -v > {fwd_out}\")\n",
    "        print(f\"  {FAST_ALIGN_BIN or 'fast_align'} -i {bitext_path} -d -o -v -r > {rev_out}\")\n",
    "        if ATOOLS_BIN:\n",
    "            print(f\"  {ATOOLS_BIN} -i {fwd_out} -j {rev_out} -c grow-diag-final-and > {sym_out}\")\n",
    "        else:\n",
    "            print(\"  (Then symmetrize with grow-diag-final using the Python code in this cell.)\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Run forward\n",
    "    cmd_fwd = [FAST_ALIGN_BIN, \"-i\", str(bitext_path), \"-d\", \"-o\", \"-v\"]\n",
    "    res_fwd = run(cmd_fwd)\n",
    "    if res_fwd.returncode != 0:\n",
    "        print(res_fwd.stderr)\n",
    "        raise RuntimeError(\"fast_align forward failed.\")\n",
    "    fwd_out.write_text(res_fwd.stdout, encoding=\"utf-8\")\n",
    "\n",
    "    # Run reverse\n",
    "    cmd_rev = [FAST_ALIGN_BIN, \"-i\", str(bitext_path), \"-d\", \"-o\", \"-v\", \"-r\"]\n",
    "    res_rev = run(cmd_rev)\n",
    "    if res_rev.returncode != 0:\n",
    "        print(res_rev.stderr)\n",
    "        raise RuntimeError(\"fast_align reverse failed.\")\n",
    "    rev_out.write_text(res_rev.stdout, encoding=\"utf-8\")\n",
    "\n",
    "    # Symmetrize\n",
    "    if ATOOLS_BIN is not None:\n",
    "        cmd_sym = [ATOOLS_BIN, \"-i\", str(fwd_out), \"-j\", str(rev_out), \"-c\", \"grow-diag-final-and\"]\n",
    "        res_sym = run(cmd_sym)\n",
    "        if res_sym.returncode != 0:\n",
    "            print(res_sym.stderr)\n",
    "            raise RuntimeError(\"atools symmetrization failed.\")\n",
    "        sym_out.write_text(res_sym.stdout, encoding=\"utf-8\")\n",
    "    else:\n",
    "        # Python grow-diag-final\n",
    "        sym_lines = []\n",
    "        with bitext_path.open(\"r\", encoding=\"utf-8\") as ftxt, \\\n",
    "             fwd_out.open(\"r\", encoding=\"utf-8\") as ff, \\\n",
    "             rev_out.open(\"r\", encoding=\"utf-8\") as fr:\n",
    "            for src_tgt, lf, lr in zip(ftxt, ff, fr):\n",
    "                src_len, tgt_len = count_tokens_pair(src_tgt)\n",
    "                F = parse_alignment_line(lf)\n",
    "                R = parse_alignment_line(lr)\n",
    "                G = grow_diag_final(F, R, src_len, tgt_len)\n",
    "                sym_lines.append(format_alignment_line(G))\n",
    "        sym_out.write_text(\"\\n\".join(sym_lines) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"[{pair_tag}] forward -> {fwd_out.name} | reverse -> {rev_out.name} | symm -> {sym_out.name}\")\n",
    "    return fwd_out, rev_out, sym_out\n",
    "\n",
    "# ====== Run alignments for both pairs ======\n",
    "fwd1, rev1, sym1 = align_pair(su_id_bitext, f\"{LANG_SRC}_{LANG_TGT_1}\")\n",
    "fwd2, rev2, sym2 = align_pair(su_en_bitext, f\"{LANG_SRC}_{LANG_TGT_2}\")\n",
    "\n",
    "# ====== Attach symmetrized links back to the sentence CSV ======\n",
    "enriched_csv = WORK_DIR / \"tri_sentences_with_align.csv\"\n",
    "if sym1 is not None and sym2 is not None:\n",
    "    # Read all alignments into memory\n",
    "    su_id_links = sym1.read_text(encoding=\"utf-8\").splitlines()\n",
    "    su_en_links = sym2.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    with IN_SENTENCES.open(\"r\", encoding=\"utf-8\") as fi, enriched_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as fo:\n",
    "        rd = csv.DictReader(fi)\n",
    "        out_fields = rd.fieldnames + [\"su_id_align\", \"su_en_align\"]\n",
    "        wr = csv.DictWriter(fo, fieldnames=out_fields)\n",
    "        wr.writeheader()\n",
    "\n",
    "        i = 0\n",
    "        for row in rd:\n",
    "            # Only increment when we actually wrote a pair (same condition as write_bitext)\n",
    "            src_ok = normalize_spaces(row.get(\"su_sentence\",\"\"))\n",
    "            id_ok  = normalize_spaces(row.get(\"id_sentence\",\"\"))\n",
    "            en_ok  = normalize_spaces(row.get(\"en_sentence\",\"\"))\n",
    "            wrote_pair1 = bool(src_ok or id_ok)\n",
    "            wrote_pair2 = bool(src_ok or en_ok)\n",
    "\n",
    "            su_id_a = su_id_links[i] if wrote_pair1 and i < len(su_id_links) else \"\"\n",
    "            su_en_a = su_en_links[i] if wrote_pair2 and i < len(su_en_links) else \"\"\n",
    "\n",
    "            if wrote_pair1 or wrote_pair2:\n",
    "                i += 1\n",
    "\n",
    "            row[\"su_id_align\"] = su_id_a\n",
    "            row[\"su_en_align\"] = su_en_a\n",
    "            wr.writerow(row)\n",
    "\n",
    "    print(f\"Enriched CSV with alignments:\\n - {enriched_csv}\")\n",
    "else:\n",
    "    print(\"\\n[INFO] fast_align not executed. Bitext files are ready; run fast_align manually,\")\n",
    "    print(\"then re-run the bottom block of this cell to attach alignments back into the CSV.\")\n",
    "    print(f\"Bitext files:\\n - {su_id_bitext}\\n - {su_en_bitext}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd76a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corpus statistics → out_preprocess\\corpus_stats.csv\n",
      "{'Language': 'su', 'Books': 66, 'Verses': 2610, 'Sentences': 4249, 'Tokens': 67699, 'Vocabulary Size': 7298}\n",
      "{'Language': 'id', 'Books': 66, 'Verses': 2809, 'Sentences': 3496, 'Tokens': 72038, 'Vocabulary Size': 5807}\n",
      "{'Language': 'en', 'Books': 66, 'Verses': 2809, 'Sentences': 3961, 'Tokens': 74430, 'Vocabulary Size': 5878}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# ====== CONFIG ======\n",
    "IN_FILE = Path(\"./out_preprocess/tri_sentences.csv\")   # from previous step\n",
    "OUT_FILE = Path(\"./out_preprocess/corpus_stats.csv\")\n",
    "\n",
    "# ====== Tokenizer ======\n",
    "import re\n",
    "TOKEN_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ']+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return [t for t in TOKEN_RE.findall(text or \"\") if t.strip()]\n",
    "\n",
    "# ====== Collect stats ======\n",
    "stats = {\n",
    "    \"su\": {\"books\": set(), \"verses\": set(), \"sentences\": 0, \"tokens\": 0, \"vocab\": Counter()},\n",
    "    \"id\": {\"books\": set(), \"verses\": set(), \"sentences\": 0, \"tokens\": 0, \"vocab\": Counter()},\n",
    "    \"en\": {\"books\": set(), \"verses\": set(), \"sentences\": 0, \"tokens\": 0, \"vocab\": Counter()},\n",
    "}\n",
    "\n",
    "with IN_FILE.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    rd = csv.DictReader(f)\n",
    "    for row in rd:\n",
    "        verse_id = row[\"verse_id\"]  # format like \"1 Korintus:1:1\"\n",
    "        parts = verse_id.split(\":\")\n",
    "        book = parts[0] if len(parts) >= 3 else \"Unknown\"\n",
    "        verse = \":\".join(parts[:3]) if len(parts) >= 3 else verse_id\n",
    "\n",
    "        for lang, col in [(\"su\",\"su_sentence\"), (\"id\",\"id_sentence\"), (\"en\",\"en_sentence\")]:\n",
    "            text = row.get(col,\"\").strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            toks = tokenize(text)\n",
    "            stats[lang][\"books\"].add(book)\n",
    "            stats[lang][\"verses\"].add(verse)\n",
    "            stats[lang][\"sentences\"] += 1\n",
    "            stats[lang][\"tokens\"] += len(toks)\n",
    "            stats[lang][\"vocab\"].update([t.lower() for t in toks])\n",
    "\n",
    "# ====== Write summary table ======\n",
    "rows = []\n",
    "for lang, st in stats.items():\n",
    "    rows.append({\n",
    "        \"Language\": lang,\n",
    "        \"Books\": len(st[\"books\"]),\n",
    "        \"Verses\": len(st[\"verses\"]),\n",
    "        \"Sentences\": st[\"sentences\"],\n",
    "        \"Tokens\": st[\"tokens\"],\n",
    "        \"Vocabulary Size\": len(st[\"vocab\"])\n",
    "    })\n",
    "\n",
    "with OUT_FILE.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    wr = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
    "    wr.writeheader()\n",
    "    wr.writerows(rows)\n",
    "\n",
    "print(f\"Saved corpus statistics → {OUT_FILE}\")\n",
    "for r in rows:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81589c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid checkpoint path: \\path\\to\\your\\comet\\checkpoint.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_checkpoint\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# replace with the local path where you stored the checkpoint\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m comet_model \u001b[38;5;241m=\u001b[39m load_from_checkpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/your/comet/checkpoint.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\comet\\models\\__init__.py:86\u001b[0m, in \u001b[0;36mload_from_checkpoint\u001b[1;34m(checkpoint_path, reload_hparams, strict, local_files_only)\u001b[0m\n\u001b[0;32m     83\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m Path(checkpoint_path)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checkpoint_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid checkpoint path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m parent_folder \u001b[38;5;241m=\u001b[39m checkpoint_path\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# .parent.parent\u001b[39;00m\n\u001b[0;32m     89\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m parent_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparams.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mException\u001b[0m: Invalid checkpoint path: \\path\\to\\your\\comet\\checkpoint.ckpt"
     ]
    }
   ],
   "source": [
    "from comet import load_from_checkpoint\n",
    "\n",
    "# replace with the local path where you stored the checkpoint\n",
    "comet_model = load_from_checkpoint(\"/path/to/your/comet/checkpoint.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bbd90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32963963f9f94b6ab31c3822d5d69c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4697861969144eb4bdf9e30debeaaef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jtkacer01\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f0a0f20e9d4e3f85d927936407e688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246e8a8b4c544eb7ab3016d7b227271b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068c98fb953f4347878b345694a04d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b56ca8feaf460b8cd66204335490da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jtkacer01\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438c967242ab413795b05e619fd7ba67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jtkacer01\\.cache\\huggingface\\hub\\models--xlm-roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd72b7029984319923ea8afbc7cbbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7060816567419fbb371c9809deb4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5900b3cff27d4e5c9ee3ea07342122da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoder model frozen.\n",
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pair</th>\n",
       "      <th>System</th>\n",
       "      <th>Unfiltered BLEU</th>\n",
       "      <th>Filtered BLEU</th>\n",
       "      <th>Weighted BLEU</th>\n",
       "      <th>Unfiltered ChrF</th>\n",
       "      <th>Filtered ChrF</th>\n",
       "      <th>Weighted ChrF</th>\n",
       "      <th>Unfiltered COMET</th>\n",
       "      <th>Filtered COMET</th>\n",
       "      <th>Weighted COMET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Pair, System, Unfiltered BLEU, Filtered BLEU, Weighted BLEU, Unfiltered ChrF, Filtered ChrF, Weighted ChrF, Unfiltered COMET, Filtered COMET, Weighted COMET]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results table → out_eval\\mt_results_table.csv\n"
     ]
    }
   ],
   "source": [
    "# MT results table: Unfiltered / Filtered / Weighted BLEU, ChrF, COMET\n",
    "# Pairs supported: su-id, su-en (source su; target id/en)\n",
    "# Weighted scores = weighted mean of sentence-level metric by 'weight' column (High=1.0, Medium=0.5, Low=0.0 by our earlier mapping)\n",
    "\n",
    "import csv, math, re, json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ======== CONFIG ========\n",
    "TEST_FLAGGED_CSV = Path(\"./out_bibtralign/1kor_trilingual_release_flagged_noted.csv\")  # or a test-only CSV\n",
    "# If your flagged CSV has a 'split' column, set USE_SPLIT=True to restrict to split=='test'\n",
    "USE_SPLIT = False\n",
    "\n",
    "# Define your systems here:\n",
    "# Each item: {\"pair\": \"su-id\" or \"su-en\", \"system\": \"Name\", \"hyp_tsv\": \"path/to/hyp.tsv\"}\n",
    "# hyp.tsv must have columns: sent_id, hyp\n",
    "SYSTEMS = [\n",
    "    # Examples (edit these paths):\n",
    "    # {\"pair\": \"su-id\", \"system\": \"Moses-SMT\", \"hyp_tsv\": \"./runs/moses_su_id_test.tsv\"},\n",
    "    # {\"pair\": \"su-id\", \"system\": \"Transformer-base\", \"hyp_tsv\": \"./runs/transformer_su_id_test.tsv\"},\n",
    "    # {\"pair\": \"su-en\", \"system\": \"Transformer-big\", \"hyp_tsv\": \"./runs/transformer_su_en_test.tsv\"},\n",
    "]\n",
    "\n",
    "# Optional: expected test size for sanity print (informational only)\n",
    "EXPECTED_TEST_SIZE = 1000\n",
    "\n",
    "# ======== Metric backends ========\n",
    "# sacrebleu for BLEU/chrF (recommended)\n",
    "try:\n",
    "    import sacrebleu\n",
    "except Exception as e:\n",
    "    sacrebleu = None\n",
    "    print(\"[WARN] sacrebleu not available. Install with: pip install sacrebleu\")\n",
    "\n",
    "# COMET (optional); if not present, we’ll output NaN\n",
    "COMET_AVAILABLE = False\n",
    "try:\n",
    "    from comet import download_model, load_from_checkpoint\n",
    "    _comet_ckpt = download_model(\"Unbabel/wmt22-comet-da\")  # good default; change if desired\n",
    "    comet_model = load_from_checkpoint(_comet_ckpt)\n",
    "    COMET_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(\"[INFO] COMET not available (or download blocked). We'll fill COMET columns with NaN.\")\n",
    "\n",
    "# ======== Helpers ========\n",
    "def _lower(s: str) -> str:\n",
    "    return (s or \"\").strip()\n",
    "\n",
    "def load_flagged_test_rows(path: Path, pair: str, use_split: bool) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    # flexible: columns may be mixed dtypes; cast where needed\n",
    "    for col in [\"weight\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
    "        else:\n",
    "            df[col] = 0.0\n",
    "    if \"flag\" not in df.columns:\n",
    "        df[\"flag\"] = \"Medium\"\n",
    "\n",
    "    # choose src/ref by pair\n",
    "    if pair == \"su-id\":\n",
    "        src_col, ref_col = \"su_sentence\", \"id_sentence\"\n",
    "    elif pair == \"su-en\":\n",
    "        src_col, ref_col = \"su_sentence\", \"en_sentence\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pair: {pair}\")\n",
    "\n",
    "    need_cols = {\"sent_id\", src_col, ref_col, \"flag\", \"weight\"}\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in flagged CSV for pair {pair}: {missing}\")\n",
    "\n",
    "    # optionally restrict to split == test\n",
    "    if use_split and \"split\" in df.columns:\n",
    "        df = df[df[\"split\"].astype(str).str.lower().eq(\"test\")].copy()\n",
    "\n",
    "    # keep only rows with a reference (non-empty)\n",
    "    df = df[df[ref_col].astype(str).str.strip().ne(\"\")].copy()\n",
    "\n",
    "    # trim strings\n",
    "    df[\"sent_id\"] = df[\"sent_id\"].astype(str).str.strip()\n",
    "    df[\"src\"] = df[src_col].astype(str).str.strip()\n",
    "    df[\"ref\"] = df[ref_col].astype(str).str.strip()\n",
    "\n",
    "    # sanity\n",
    "    if EXPECTED_TEST_SIZE:\n",
    "        print(f\"[{pair}] Loaded {len(df)} test rows (expected ~{EXPECTED_TEST_SIZE}).\")\n",
    "\n",
    "    return df[[\"sent_id\",\"src\",\"ref\",\"flag\",\"weight\"]].copy()\n",
    "\n",
    "def load_hyp_tsv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
    "    # allow comma-separated too\n",
    "    if \"sent_id\" not in df.columns or \"hyp\" not in df.columns:\n",
    "        # try CSV fallback\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "    assert \"sent_id\" in df.columns and \"hyp\" in df.columns, f\"{path} must have columns: sent_id, hyp\"\n",
    "    df[\"sent_id\"] = df[\"sent_id\"].astype(str).str.strip()\n",
    "    df[\"hyp\"] = df[\"hyp\"].astype(str).str.strip()\n",
    "    return df[[\"sent_id\",\"hyp\"]].copy()\n",
    "\n",
    "def join_test_hyp(test_df: pd.DataFrame, hyp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    merged = test_df.merge(hyp_df, on=\"sent_id\", how=\"inner\")\n",
    "    missing = len(test_df) - len(merged)\n",
    "    if missing:\n",
    "        print(f\"[WARN] {missing} test rows had no hypothesis and were dropped after join.\")\n",
    "    return merged\n",
    "\n",
    "def compute_bleu_chrf_lists(refs: List[str], hyps: List[str]) -> Tuple[float, float, List[float], List[float]]:\n",
    "    \"\"\"Return corpus BLEU/chrF and sentence-level lists for weighted averaging.\"\"\"\n",
    "    if sacrebleu is None:\n",
    "        return float(\"nan\"), float(\"nan\"), [], []\n",
    "    # corpus\n",
    "    bleu_corpus = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    chrf_corpus = sacrebleu.corpus_chrf(hyps, [refs]).score\n",
    "    # sentence-level (for weighted)\n",
    "    bleu_sent, chrf_sent = [], []\n",
    "    for h, r in zip(hyps, refs):\n",
    "        bleu_sent.append(sacrebleu.sentence_bleu(h, [r]).score)\n",
    "        chrf_sent.append(sacrebleu.sentence_chrf(h, [r]).score)\n",
    "    return bleu_corpus, chrf_corpus, bleu_sent, chrf_sent\n",
    "\n",
    "def weighted_mean(scores: List[float], weights: List[float]) -> float:\n",
    "    num = sum(s*w for s, w in zip(scores, weights))\n",
    "    den = sum(weights)\n",
    "    return (num/den) if den > 0 else float(\"nan\")\n",
    "\n",
    "def compute_comet(srcs: List[str], hyps: List[str], refs: List[str]) -> Tuple[float, List[float]]:\n",
    "    \"\"\"Return corpus mean COMET and sentence-level scores; NaN if COMET unavailable.\"\"\"\n",
    "    if not COMET_AVAILABLE:\n",
    "        return float(\"nan\"), []\n",
    "    data = [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(srcs, hyps, refs)]\n",
    "    out = comet_model.predict(data, batch_size=32, gpus=0)\n",
    "    sent_scores = out[\"scores\"]\n",
    "    corpus = float(sum(sent_scores) / max(1, len(sent_scores)))\n",
    "    return corpus, sent_scores\n",
    "\n",
    "def evaluate_split(df: pd.DataFrame, use_filter: bool, use_weighted: bool) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    df columns: sent_id, src, ref, flag, weight, hyp\n",
    "    - Unfiltered: use all rows\n",
    "    - Filtered: use only flag=='High'\n",
    "    - Weighted: weight column over sentence-level metrics (not true corpus BLEU)\n",
    "    \"\"\"\n",
    "    if use_filter:\n",
    "        df = df[df[\"flag\"].astype(str).str.lower().eq(\"high\")].copy()\n",
    "    if len(df) == 0:\n",
    "        return {\"BLEU\": float(\"nan\"), \"ChrF\": float(\"nan\"), \"COMET\": float(\"nan\")}\n",
    "\n",
    "    refs = df[\"ref\"].tolist()\n",
    "    hyps = df[\"hyp\"].tolist()\n",
    "    srcs = df[\"src\"].tolist()\n",
    "    weights = df[\"weight\"].astype(float).tolist()\n",
    "\n",
    "    bleu_corpus, chrf_corpus, bleu_sent, chrf_sent = compute_bleu_chrf_lists(refs, hyps)\n",
    "    comet_corpus, comet_sent = compute_comet(srcs, hyps, refs)\n",
    "\n",
    "    if use_weighted:\n",
    "        # weighted average of sentence-level scores\n",
    "        bleu = weighted_mean(bleu_sent, weights)\n",
    "        chrf = weighted_mean(chrf_sent, weights)\n",
    "        comet = weighted_mean(comet_sent, weights) if comet_sent else float(\"nan\")\n",
    "    else:\n",
    "        # corpus scores\n",
    "        bleu, chrf, comet = bleu_corpus, chrf_corpus, comet_corpus\n",
    "\n",
    "    return {\"BLEU\": bleu, \"ChrF\": chrf, \"COMET\": comet}\n",
    "\n",
    "# ======== Main: build the table ========\n",
    "rows_out = []\n",
    "\n",
    "for spec in SYSTEMS:\n",
    "    pair = spec[\"pair\"].strip().lower()\n",
    "    system_name = spec[\"system\"]\n",
    "    hyp_path = Path(spec[\"hyp_tsv\"])\n",
    "\n",
    "    # Load test rows for pair, then join hypothesis\n",
    "    test_df = load_flagged_test_rows(TEST_FLAGGED_CSV, pair, USE_SPLIT)\n",
    "    hyp_df = load_hyp_tsv(hyp_path)\n",
    "    merged = join_test_hyp(test_df, hyp_df)\n",
    "\n",
    "    # Unfiltered / Filtered / Weighted\n",
    "    unfiltered = evaluate_split(merged, use_filter=False, use_weighted=False)\n",
    "    filtered   = evaluate_split(merged, use_filter=True,  use_weighted=False)\n",
    "    weighted   = evaluate_split(merged, use_filter=False, use_weighted=True)\n",
    "\n",
    "    rows_out.append({\n",
    "        \"Pair\": pair,\n",
    "        \"System\": system_name,\n",
    "        \"Unfiltered BLEU\": round(unfiltered[\"BLEU\"], 2) if pd.notna(unfiltered[\"BLEU\"]) else float(\"nan\"),\n",
    "        \"Filtered BLEU\":   round(filtered[\"BLEU\"],   2) if pd.notna(filtered[\"BLEU\"])   else float(\"nan\"),\n",
    "        \"Weighted BLEU\":   round(weighted[\"BLEU\"],   2) if pd.notna(weighted[\"BLEU\"])   else float(\"nan\"),\n",
    "        \"Unfiltered ChrF\": round(unfiltered[\"ChrF\"], 2) if pd.notna(unfiltered[\"ChrF\"]) else float(\"nan\"),\n",
    "        \"Filtered ChrF\":   round(filtered[\"ChrF\"],   2) if pd.notna(filtered[\"ChrF\"])   else float(\"nan\"),\n",
    "        \"Weighted ChrF\":   round(weighted[\"ChrF\"],   2) if pd.notna(weighted[\"ChrF\"])   else float(\"nan\"),\n",
    "        \"Unfiltered COMET\":round(unfiltered[\"COMET\"],3) if pd.notna(unfiltered[\"COMET\"])else float(\"nan\"),\n",
    "        \"Filtered COMET\":  round(filtered[\"COMET\"],  3) if pd.notna(filtered[\"COMET\"])  else float(\"nan\"),\n",
    "        \"Weighted COMET\":  round(weighted[\"COMET\"],  3) if pd.notna(weighted[\"COMET\"])  else float(\"nan\"),\n",
    "    })\n",
    "\n",
    "# Output table\n",
    "df_out = pd.DataFrame(rows_out, columns=[\n",
    "    \"Pair\",\"System\",\n",
    "    \"Unfiltered BLEU\",\"Filtered BLEU\",\"Weighted BLEU\",\n",
    "    \"Unfiltered ChrF\",\"Filtered ChrF\",\"Weighted ChrF\",\n",
    "    \"Unfiltered COMET\",\"Filtered COMET\",\"Weighted COMET\",\n",
    "])\n",
    "\n",
    "save_path = Path(\"./out_eval/mt_results_table.csv\")\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
    "display(df_out)\n",
    "print(f\"\\nSaved results table → {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28560061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4bda8955104ca285faac8c7fcb069a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jtkacer01\\.cache\\huggingface\\hub\\models--sentence-transformers--LaBSE. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d1c07f829b45ddae03b783ed458894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a07fd7678d41a3bb65db199a5687ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf81bfa748f4e75ac79c1a4401a9823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980db98126634eb1afa1f37fc3b41c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419ee3694fa140aca5a77942a26c9b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e69996da5040338a6205ef446235ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cb55b32a8e494394dced22e0695a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d0add8f75f4eb996169a98dc55a71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e841feddbd4b4ebcb73dcd8a6c147e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162ca375cb3a4d8b854bc642666f07be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba76f2d480d48a7a7e93fdbeb5747bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfe94eda9634ddb92661e39943eb5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded model: sentence-transformers/LaBSE\n",
      "[su-id] Wrote 1803 links → out_align_py\\su_id_emb_align.csv\n",
      "[su-en] Wrote 1467 links → out_align_py\\su_en_emb_align.csv\n",
      "Wrote per-row flags → out_align_py\\tri_sentences_with_embflag.csv\n",
      "flag_su_id_emb counts: {'High': 276, 'Medium': 1097, '': 3072, 'Low': 430}\n",
      "flag_su_en_emb counts: {'High': 211, 'Low': 234, 'Medium': 1022, '': 3408}\n"
     ]
    }
   ],
   "source": [
    "# Python-only alignment and quality flags using multilingual sentence embeddings (LaBSE fallback)\n",
    "# - Aligns within each verse: su-sentences ↔ id-sentences and su-sentences ↔ en-sentences\n",
    "# - Greedy max-sim matching with threshold\n",
    "# - Adds heuristic features (length ratio, punctuation diff, NE overlap) and flags High/Medium/Low\n",
    "# - Writes: su_id_emb_align.csv, su_en_emb_align.csv, tri_sentences_with_embflag.csv\n",
    "\n",
    "import os, re, math, csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ======== CONFIG ========\n",
    "IN_SENT = Path(\"./out_preprocess/tri_sentences.csv\")\n",
    "OUT_DIR = Path(\"./out_align_py\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Embedding model preference list (first available will be used)\n",
    "# LaBSE is best; the MiniLM model is small and downloads fast if needed.\n",
    "PREFERRED_MODELS = [\n",
    "    \"sentence-transformers/LaBSE\",\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "]\n",
    "\n",
    "DEVICE = \"cpu\"            # \"cuda\" if you have a GPU\n",
    "BATCH_SIZE = 64\n",
    "SIM_THRESH_HIGH = 0.80    # cosine similarity threshold for \"High\"\n",
    "SIM_THRESH_MED  = 0.60    # threshold for \"Medium\" (between MED and HIGH)\n",
    "LEN_GOOD_SUID   = 0.70    # length ratio good su↔id\n",
    "LEN_GOOD_SUEN   = 0.60    # length ratio good su↔en\n",
    "\n",
    "# ======== Tokenization / helpers ========\n",
    "TOKEN_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ0-9']+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return [t for t in TOKEN_RE.findall(s or \"\") if t.strip()]\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(tokenize(s))\n",
    "\n",
    "def punct_profile(s: str):\n",
    "    return (s.count(\",\"), s.count(\";\"), s.count(\":\"), s.count(\"?\"), s.count(\"!\"))\n",
    "\n",
    "NAME_VARIANTS = [\n",
    "    {\"abraham\",\"ibrahim\"},\n",
    "    {\"isaac\",\"ishak\",\"ishaq\"},\n",
    "    {\"yakub\",\"yakob\",\"jacob\"},\n",
    "    {\"musa\",\"moses\"},\n",
    "    {\"daud\",\"david\"},\n",
    "    {\"yohanes\",\"john\"},\n",
    "    {\"matheus\",\"mateus\",\"matthew\"},\n",
    "    {\"yesus\",\"jesus\"},\n",
    "    {\"petros\",\"petrus\",\"peter\"},\n",
    "]\n",
    "\n",
    "def name_set_loose(s: str):\n",
    "    toks = set(t.lower() for t in tokenize(s) if t.isalpha())\n",
    "    present = set()\n",
    "    for i, group in enumerate(NAME_VARIANTS):\n",
    "        if toks & group:\n",
    "            present.add(i)\n",
    "    return present\n",
    "\n",
    "def ne_overlap(su: str, tgt: str) -> float:\n",
    "    gs, gt = name_set_loose(su), name_set_loose(tgt)\n",
    "    if not gs and not gt:\n",
    "        return 1.0\n",
    "    if not gs or not gt:\n",
    "        return 0.0\n",
    "    inter = len(gs & gt)\n",
    "    union = len(gs | gt)\n",
    "    return inter / max(1, union)\n",
    "\n",
    "def len_ratio(a: str, b: str) -> float:\n",
    "    la, lb = token_len(a), token_len(b)\n",
    "    if la == 0 or lb == 0: return 0.0\n",
    "    return min(la, lb) / max(la, lb)\n",
    "\n",
    "def punct_diff(su: str, tgt: str) -> int:\n",
    "    ps, pt = punct_profile(su), punct_profile(tgt)\n",
    "    return sum(abs(a-b) for a,b in zip(ps, pt))\n",
    "\n",
    "def flag_from_metrics(sim: float, lratio: float, p_diff: int, ne_ov: float, pair: str) -> str:\n",
    "    # Pair-specific good length threshold\n",
    "    len_good = LEN_GOOD_SUID if pair == \"su-id\" else LEN_GOOD_SUEN\n",
    "    if sim >= SIM_THRESH_HIGH and lratio >= len_good and p_diff <= 1 and ne_ov >= 0.8:\n",
    "        return \"High\"\n",
    "    if sim >= SIM_THRESH_MED and lratio >= 0.5 and p_diff <= 2 and ne_ov >= 0.3:\n",
    "        return \"Medium\"\n",
    "    return \"Low\"\n",
    "\n",
    "# ======== Embedding backend ========\n",
    "def load_embedder():\n",
    "    global st_model\n",
    "    st_model = None\n",
    "    err = None\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Please install sentence-transformers: pip install sentence-transformers\") from e\n",
    "\n",
    "    for m in PREFERRED_MODELS:\n",
    "        try:\n",
    "            st_model = SentenceTransformer(m, device=DEVICE)\n",
    "            print(f\"[INFO] Loaded model: {m}\")\n",
    "            return st_model\n",
    "        except Exception as e:\n",
    "            err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Could not load any embedding model from {PREFERRED_MODELS}. Error: {err}\")\n",
    "\n",
    "def embed_texts(texts: list) -> np.ndarray:\n",
    "    # sentence-transformers returns numpy array\n",
    "    if not texts:\n",
    "        return np.zeros((0, 768), dtype=\"float32\")\n",
    "    embs = st_model.encode(texts, batch_size=BATCH_SIZE, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embs\n",
    "\n",
    "def cosine_sim_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    # A, B are L2-normalized; cosine = dot\n",
    "    return np.matmul(A, B.T)\n",
    "\n",
    "def greedy_max_match(sim: np.ndarray, thresh: float) -> list:\n",
    "    \"\"\"\n",
    "    Greedy bipartite matching on similarity matrix.\n",
    "    Returns list of (i, j, sim_ij) with sim_ij >= thresh. Each row/col matched at most once.\n",
    "    \"\"\"\n",
    "    if sim.size == 0:\n",
    "        return []\n",
    "    S = sim.copy()\n",
    "    pairs = []\n",
    "    used_r = set()\n",
    "    used_c = set()\n",
    "    # Flatten indices sorted by similarity descending\n",
    "    flat = np.dstack(np.unravel_index(np.argsort(-S, axis=None), S.shape))[0]\n",
    "    for r,c in flat:\n",
    "        if r in used_r or c in used_c:\n",
    "            continue\n",
    "        s = S[r, c]\n",
    "        if s < thresh:\n",
    "            break\n",
    "        pairs.append((int(r), int(c), float(s)))\n",
    "        used_r.add(int(r))\n",
    "        used_c.add(int(c))\n",
    "    return pairs\n",
    "\n",
    "# ======== Load sentence data ========\n",
    "assert IN_SENT.exists(), f\"Missing input: {IN_SENT}\"\n",
    "df = pd.read_csv(IN_SENT, dtype=str).fillna(\"\")\n",
    "need = {\"verse_id\",\"sent_idx\",\"su_sentence\",\"id_sentence\",\"en_sentence\"}\n",
    "missing = need - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"tri_sentences.csv must contain columns {sorted(need)}; missing {sorted(missing)}\")\n",
    "\n",
    "# group by verse to align within verse\n",
    "groups = defaultdict(list)\n",
    "for row in df.to_dict(\"records\"):\n",
    "    groups[row[\"verse_id\"]].append(row)\n",
    "\n",
    "# ======== Load model ========\n",
    "st_model = load_embedder()\n",
    "\n",
    "# ======== Align function per pair ========\n",
    "def align_pair(pair: str, src_col: str, tgt_col: str, sim_thresh: float):\n",
    "    align_rows = []  # per-link table\n",
    "    per_row_flags = {}  # sent_id -> flag for best aligned target (only for reporting back)\n",
    "\n",
    "    for verse_id, rows in groups.items():\n",
    "        # collect per-verse sentence lists\n",
    "        src_items = [(int(r[\"sent_idx\"]), r[src_col]) for r in rows if r[src_col].strip()]\n",
    "        tgt_items = [(int(r[\"sent_idx\"]), r[tgt_col]) for r in rows if r[tgt_col].strip()]\n",
    "        if not src_items or not tgt_items:\n",
    "            continue\n",
    "\n",
    "        src_idx, src_texts = zip(*src_items)\n",
    "        tgt_idx, tgt_texts = zip(*tgt_items)\n",
    "\n",
    "        # embeddings + sim matrix\n",
    "        E_src = embed_texts(list(src_texts))\n",
    "        E_tgt = embed_texts(list(tgt_texts))\n",
    "        S = cosine_sim_matrix(E_src, E_tgt)\n",
    "\n",
    "        # greedy matching above minimum sim threshold (use MED)\n",
    "        pairs = greedy_max_match(S, SIM_THRESH_MED)\n",
    "\n",
    "        # record links (and compute heuristics)\n",
    "        for (i_local, j_local, sim_val) in pairs:\n",
    "            i = src_idx[i_local]\n",
    "            j = tgt_idx[j_local]\n",
    "            su = src_texts[i_local]  # src is su_sentence by design\n",
    "            tg = tgt_texts[j_local]\n",
    "\n",
    "            lr = len_ratio(su, tg)\n",
    "            pdiff = punct_diff(su, tg)\n",
    "            neo = ne_overlap(su, tg)\n",
    "            flag = flag_from_metrics(sim_val, lr, pdiff, neo, pair)\n",
    "\n",
    "            align_rows.append({\n",
    "                \"verse_id\": verse_id,\n",
    "                \"pair\": pair,\n",
    "                \"su_idx\": i,\n",
    "                \"tgt_idx\": j,\n",
    "                \"su_text\": su,\n",
    "                \"tgt_text\": tg,\n",
    "                \"sim\": round(sim_val, 4),\n",
    "                \"len_ratio\": round(lr, 3),\n",
    "                \"punct_diff\": int(pdiff),\n",
    "                \"ne_overlap\": round(neo, 3),\n",
    "                \"flag\": flag\n",
    "            })\n",
    "\n",
    "            # set per-row best flag back to sentence id (we keep max by sim)\n",
    "            sent_id = f\"{verse_id}#{i}\"\n",
    "            if sent_id not in per_row_flags or sim_val > per_row_flags[sent_id][0]:\n",
    "                per_row_flags[sent_id] = (sim_val, flag)\n",
    "\n",
    "    # Save link table\n",
    "    out_links = OUT_DIR / f\"{pair.replace('-','_')}_emb_align.csv\"\n",
    "    pd.DataFrame(align_rows,\n",
    "                 columns=[\"verse_id\",\"pair\",\"su_idx\",\"tgt_idx\",\"su_text\",\"tgt_text\",\"sim\",\"len_ratio\",\"punct_diff\",\"ne_overlap\",\"flag\"])\\\n",
    "      .to_csv(out_links, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"[{pair}] Wrote {len(align_rows)} links → {out_links}\")\n",
    "    return per_row_flags\n",
    "\n",
    "# ======== Run for both pairs ========\n",
    "su_id_flags = align_pair(\"su-id\", \"su_sentence\", \"id_sentence\", SIM_THRESH_MED)\n",
    "su_en_flags = align_pair(\"su-en\", \"su_sentence\", \"en_sentence\", SIM_THRESH_MED)\n",
    "\n",
    "# ======== Attach per-row flags back to tri_sentences ========\n",
    "def best_flag(sent_id: str, mapping: dict):\n",
    "    if sent_id in mapping:\n",
    "        return mapping[sent_id][1]\n",
    "    return \"\"\n",
    "\n",
    "df[\"sent_id\"] = df[\"verse_id\"].astype(str) + \"#\" + df[\"sent_idx\"].astype(str)\n",
    "df[\"flag_su_id_emb\"] = df[\"sent_id\"].map(lambda x: best_flag(x, su_id_flags))\n",
    "df[\"flag_su_en_emb\"] = df[\"sent_id\"].map(lambda x: best_flag(x, su_en_flags))\n",
    "\n",
    "out_sent = OUT_DIR / \"tri_sentences_with_embflag.csv\"\n",
    "df.to_csv(out_sent, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote per-row flags → {out_sent}\")\n",
    "\n",
    "# Small summary\n",
    "for col in [\"flag_su_id_emb\",\"flag_su_en_emb\"]:\n",
    "    vc = Counter(df[col].fillna(\"\").tolist())\n",
    "    print(f\"{col} counts:\", dict(vc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83296b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      " - out_eval_ready\\eval_test_su_id.csv\n",
      " - out_eval_ready\\eval_test_su_en.csv\n",
      "[su-id] rows=4875, test=1000, flags={'': 3072, 'Medium': 1097, 'Low': 430, 'High': 276}\n",
      "[su-en] rows=4875, test=1000, flags={'': 3408, 'Medium': 1022, 'Low': 234, 'High': 211}\n",
      "Templates written:\n",
      " - out_eval_ready\\test_template_su_id.tsv\n",
      " - out_eval_ready\\test_template_su_en.tsv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ========= CONFIG =========\n",
    "IN_SENT_FLAGS = Path(\"./out_align_py/tri_sentences_with_embflag.csv\")\n",
    "OUT_DIR       = Path(\"./out_eval_ready\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# How to combine multiple sources of weights (if you later add others):\n",
    "AGGREGATION = \"identity\"  # one of: \"identity\", \"max\", \"mean\", \"min\"\n",
    "# If you want to produce a deterministic Test split:\n",
    "CREATE_SPLIT = True\n",
    "TEST_SIZE    = 1000       # set None to skip rebalancing by size\n",
    "\n",
    "# Weight mapping\n",
    "WEIGHT_MAP = {\"High\": 1.0, \"Medium\": 0.5, \"Low\": 0.0, \"\": 0.0}\n",
    "\n",
    "# ========= Load =========\n",
    "assert IN_SENT_FLAGS.exists(), f\"Missing: {IN_SENT_FLAGS}\"\n",
    "df = pd.read_csv(IN_SENT_FLAGS, dtype=str).fillna(\"\")\n",
    "\n",
    "needed_cols = {\n",
    "    \"verse_id\",\"sent_idx\",\"sent_id\",\n",
    "    \"su_sentence\",\"id_sentence\",\"en_sentence\",\n",
    "    \"flag_su_id_emb\",\"flag_su_en_emb\"\n",
    "}\n",
    "missing = needed_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Input must include columns {sorted(needed_cols)}; missing: {sorted(missing)}\")\n",
    "\n",
    "# ========= Map flags -> weights =========\n",
    "def to_weight(flag: str) -> float:\n",
    "    return WEIGHT_MAP.get(str(flag).strip(), 0.0)\n",
    "\n",
    "df[\"weight_su_id_emb\"] = df[\"flag_su_id_emb\"].map(to_weight)\n",
    "df[\"weight_su_en_emb\"] = df[\"flag_su_en_emb\"].map(to_weight)\n",
    "\n",
    "# If you ever want a single \"weight\" column aggregated from multiple sources, choose here:\n",
    "def agg_weights(row, pair: str):\n",
    "    if pair == \"su-id\":\n",
    "        vals = [row[\"weight_su_id_emb\"]]\n",
    "    elif pair == \"su-en\":\n",
    "        vals = [row[\"weight_su_en_emb\"]]\n",
    "    else:\n",
    "        vals = []\n",
    "    if not vals:\n",
    "        return 0.0\n",
    "    if AGGREGATION == \"max\":\n",
    "        return max(vals)\n",
    "    if AGGREGATION == \"mean\":\n",
    "        return float(sum(vals) / len(vals))\n",
    "    if AGGREGATION == \"min\":\n",
    "        return min(vals)\n",
    "    # identity → the one we already chose per pair\n",
    "    return vals[0]\n",
    "\n",
    "# ========= Build pair-specific eval tables =========\n",
    "def build_eval_pair(df_src: pd.DataFrame, pair: str) -> pd.DataFrame:\n",
    "    if pair == \"su-id\":\n",
    "        tgt_col = \"id_sentence\"\n",
    "        flag_col = \"flag_su_id_emb\"\n",
    "        wcol = \"weight_su_id_emb\"\n",
    "    elif pair == \"su-en\":\n",
    "        tgt_col = \"en_sentence\"\n",
    "        flag_col = \"flag_su_en_emb\"\n",
    "        wcol = \"weight_su_en_emb\"\n",
    "    else:\n",
    "        raise ValueError(\"pair must be 'su-id' or 'su-en'\")\n",
    "\n",
    "    sub = df_src[[\"sent_id\",\"su_sentence\",tgt_col,flag_col,wcol]].copy()\n",
    "    sub.columns = [\"sent_id\",\"src\",\"ref\",\"flag\",\"weight\"]\n",
    "\n",
    "    # ensure string flags\n",
    "    sub[\"flag\"] = sub[\"flag\"].astype(str)\n",
    "\n",
    "    if CREATE_SPLIT:\n",
    "        # map flags to rank: High > Medium > Low\n",
    "        sub[\"order_key\"] = sub[\"flag\"].map({\"High\": 2, \"Medium\": 1, \"Low\": 0}).fillna(0).astype(float)\n",
    "        # sort by the **column name**, not the Series object\n",
    "        sub = sub.sort_values(by=[\"order_key\", \"sent_id\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "        if TEST_SIZE is not None:\n",
    "            TEST_SIZE_INT = int(TEST_SIZE)\n",
    "            sub[\"split\"] = [\"test\" if i < TEST_SIZE_INT else \"train\" for i in range(len(sub))]\n",
    "        else:\n",
    "            sub[\"split\"] = \"test\"\n",
    "        # optional: drop helper column\n",
    "        sub = sub.drop(columns=[\"order_key\"])\n",
    "    else:\n",
    "        sub[\"split\"] = \"\"\n",
    "\n",
    "    return sub\n",
    "\n",
    "eval_su_id = build_eval_pair(df, \"su-id\")\n",
    "eval_su_en = build_eval_pair(df, \"su-en\")\n",
    "\n",
    "out_id = OUT_DIR / \"eval_test_su_id.csv\"\n",
    "out_en = OUT_DIR / \"eval_test_su_en.csv\"\n",
    "eval_su_id.to_csv(out_id, index=False, encoding=\"utf-8\")\n",
    "eval_su_en.to_csv(out_en, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" -\", out_id)\n",
    "print(\" -\", out_en)\n",
    "\n",
    "# Show quick summaries\n",
    "for name, sub in [(\"su-id\", eval_su_id), (\"su-en\", eval_su_en)]:\n",
    "    cnt = len(sub)\n",
    "    test_cnt = (sub[\"split\"] == \"test\").sum()\n",
    "    flag_counts = sub[\"flag\"].value_counts().to_dict()\n",
    "    print(f\"[{name}] rows={cnt}, test={test_cnt}, flags={flag_counts}\")\n",
    "\n",
    "# ========= (Optional) Tiny helper to create a 'sent_id,hyp' TSV template for systems =========\n",
    "# This lets you hand a test list to your MT system to preserve sent_id ordering.\n",
    "template_id = OUT_DIR / \"test_template_su_id.tsv\"\n",
    "template_en = OUT_DIR / \"test_template_su_en.tsv\"\n",
    "eval_su_id.loc[eval_su_id[\"split\"].eq(\"test\"), [\"sent_id\"]].assign(hyp=\"\").to_csv(template_id, sep=\"\\t\", index=False)\n",
    "eval_su_en.loc[eval_su_en[\"split\"].eq(\"test\"), [\"sent_id\"]].assign(hyp=\"\").to_csv(template_en, sep=\"\\t\", index=False)\n",
    "print(\"Templates written:\")\n",
    "print(\" -\", template_id)\n",
    "print(\" -\", template_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c0e73b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (4.56.2)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\jtkacer01\\\\anaconda3\\\\Lib\\\\site-packages\\\\~orch\\\\lib\\\\asmjit.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/5f/a0/d9ef19f780f319c21ee90ecfef4431cbeeca95bec7f14071785c17b6029b/accelerate-1.10.1-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/84/57/2f64161769610cf6b1c5ed782bd8a780e18a3c9d48931319f2887fa9d0b1/torch-2.8.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.8.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jtkacer01\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "   ---------------------------------------- 0.0/374.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 92.2/374.9 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 374.9/374.9 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading torch-2.8.0-cp311-cp311-win_amd64.whl (241.4 MB)\n",
      "   ---------------------------------------- 0.0/241.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.4/241.4 MB 28.0 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.7/241.4 MB 7.7 MB/s eta 0:00:32\n",
      "   ---------------------------------------- 1.0/241.4 MB 9.1 MB/s eta 0:00:27\n",
      "   ---------------------------------------- 1.3/241.4 MB 7.1 MB/s eta 0:00:34\n",
      "   ---------------------------------------- 1.8/241.4 MB 7.7 MB/s eta 0:00:32\n",
      "   ---------------------------------------- 2.2/241.4 MB 7.7 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 2.8/241.4 MB 8.6 MB/s eta 0:00:28\n",
      "   ---------------------------------------- 2.8/241.4 MB 8.6 MB/s eta 0:00:28\n",
      "    --------------------------------------- 3.2/241.4 MB 7.3 MB/s eta 0:00:33\n",
      "    --------------------------------------- 3.8/241.4 MB 8.1 MB/s eta 0:00:30\n",
      "    --------------------------------------- 4.3/241.4 MB 8.0 MB/s eta 0:00:30\n",
      "    --------------------------------------- 4.7/241.4 MB 8.0 MB/s eta 0:00:30\n",
      "    --------------------------------------- 4.9/241.4 MB 7.6 MB/s eta 0:00:32\n",
      "    --------------------------------------- 5.4/241.4 MB 7.8 MB/s eta 0:00:31\n",
      "    --------------------------------------- 5.7/241.4 MB 7.8 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 6.0/241.4 MB 7.7 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 6.7/241.4 MB 8.0 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 6.9/241.4 MB 7.7 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 7.4/241.4 MB 7.9 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 7.7/241.4 MB 7.8 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 8.2/241.4 MB 7.9 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 8.6/241.4 MB 8.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 8.9/241.4 MB 7.9 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 9.5/241.4 MB 8.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 9.5/241.4 MB 8.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 10.2/241.4 MB 8.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 10.7/241.4 MB 8.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 10.8/241.4 MB 8.0 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 11.3/241.4 MB 8.2 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 11.8/241.4 MB 8.3 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 11.9/241.4 MB 8.0 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 12.5/241.4 MB 8.2 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 12.8/241.4 MB 8.1 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 13.4/241.4 MB 8.5 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 13.7/241.4 MB 8.3 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 14.1/241.4 MB 8.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 14.8/241.4 MB 8.3 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 15.0/241.4 MB 8.3 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 15.6/241.4 MB 8.5 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 16.0/241.4 MB 8.8 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 16.2/241.4 MB 8.8 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 16.7/241.4 MB 8.6 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 17.1/241.4 MB 8.8 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 17.8/241.4 MB 8.8 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 18.0/241.4 MB 8.7 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 18.5/241.4 MB 8.7 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 19.0/241.4 MB 9.0 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 19.3/241.4 MB 8.8 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 19.9/241.4 MB 9.2 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 20.2/241.4 MB 9.0 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 20.9/241.4 MB 9.0 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 21.0/241.4 MB 9.1 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 21.6/241.4 MB 9.1 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 22.0/241.4 MB 8.8 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 22.7/241.4 MB 9.4 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 23.2/241.4 MB 9.5 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 23.3/241.4 MB 9.5 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 23.5/241.4 MB 8.8 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 24.1/241.4 MB 9.2 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 24.7/241.4 MB 9.2 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 25.1/241.4 MB 9.1 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 25.2/241.4 MB 9.0 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 25.5/241.4 MB 8.8 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 25.9/241.4 MB 8.7 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 26.5/241.4 MB 9.1 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 26.6/241.4 MB 9.1 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 26.6/241.4 MB 9.1 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 27.1/241.4 MB 8.5 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 27.7/241.4 MB 8.5 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 28.4/241.4 MB 8.8 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 28.8/241.4 MB 8.7 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 29.1/241.4 MB 8.6 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 29.5/241.4 MB 8.5 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 29.7/241.4 MB 8.5 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 29.8/241.4 MB 8.5 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 30.2/241.4 MB 8.3 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 30.8/241.4 MB 8.2 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 31.6/241.4 MB 8.5 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 31.8/241.4 MB 8.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 32.5/241.4 MB 8.5 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 32.8/241.4 MB 8.3 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 33.1/241.4 MB 8.2 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 33.7/241.4 MB 8.6 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 33.9/241.4 MB 8.7 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 34.3/241.4 MB 8.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 35.0/241.4 MB 8.5 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 35.5/241.4 MB 8.8 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 35.8/241.4 MB 8.7 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 36.2/241.4 MB 8.8 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 36.5/241.4 MB 8.7 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 36.9/241.4 MB 9.4 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 36.9/241.4 MB 9.4 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 37.5/241.4 MB 9.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 38.2/241.4 MB 9.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 38.6/241.4 MB 8.7 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 38.9/241.4 MB 8.8 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 39.2/241.4 MB 8.7 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 39.7/241.4 MB 8.7 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 40.2/241.4 MB 9.2 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 40.4/241.4 MB 9.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 41.0/241.4 MB 9.2 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 41.6/241.4 MB 9.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 41.7/241.4 MB 9.1 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 42.0/241.4 MB 8.7 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 42.5/241.4 MB 8.7 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 42.7/241.4 MB 8.7 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 43.2/241.4 MB 8.7 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 43.8/241.4 MB 8.7 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 43.9/241.4 MB 8.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 44.5/241.4 MB 8.7 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 45.0/241.4 MB 8.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 45.0/241.4 MB 8.5 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 45.4/241.4 MB 8.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 45.8/241.4 MB 8.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 46.4/241.4 MB 8.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 46.5/241.4 MB 8.2 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 47.0/241.4 MB 8.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 47.4/241.4 MB 8.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 47.7/241.4 MB 8.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 48.2/241.4 MB 8.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 48.2/241.4 MB 8.3 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 48.7/241.4 MB 8.0 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 49.4/241.4 MB 8.4 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 49.8/241.4 MB 8.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 49.8/241.4 MB 8.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 50.2/241.4 MB 8.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 50.4/241.4 MB 8.2 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 51.0/241.4 MB 8.2 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 51.3/241.4 MB 8.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 51.9/241.4 MB 8.0 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 52.3/241.4 MB 8.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 52.6/241.4 MB 8.2 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 53.2/241.4 MB 8.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 53.6/241.4 MB 8.4 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 53.8/241.4 MB 8.2 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 54.1/241.4 MB 8.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 54.4/241.4 MB 8.2 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 55.0/241.4 MB 8.3 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 55.3/241.4 MB 8.7 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 55.9/241.4 MB 8.7 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 56.3/241.4 MB 8.5 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 56.6/241.4 MB 8.3 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 56.7/241.4 MB 8.2 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 56.7/241.4 MB 8.0 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 57.4/241.4 MB 8.2 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 57.9/241.4 MB 8.3 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 58.6/241.4 MB 8.5 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 58.7/241.4 MB 8.5 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 58.7/241.4 MB 8.5 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 59.0/241.4 MB 7.9 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 59.6/241.4 MB 7.8 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 59.8/241.4 MB 7.5 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 60.4/241.4 MB 8.0 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 60.5/241.4 MB 7.9 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 60.7/241.4 MB 7.6 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 61.1/241.4 MB 7.4 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 61.5/241.4 MB 7.5 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 62.2/241.4 MB 7.6 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 62.6/241.4 MB 7.6 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 63.1/241.4 MB 7.7 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 63.2/241.4 MB 7.4 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 63.8/241.4 MB 7.4 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 63.9/241.4 MB 7.6 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 64.4/241.4 MB 7.7 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 64.9/241.4 MB 7.6 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 64.9/241.4 MB 7.4 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 65.5/241.4 MB 7.4 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 65.9/241.4 MB 7.4 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 66.0/241.4 MB 7.4 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 66.2/241.4 MB 7.0 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 66.3/241.4 MB 6.8 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 66.4/241.4 MB 6.7 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 66.6/241.4 MB 6.7 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 66.6/241.4 MB 6.6 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 66.6/241.4 MB 6.6 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 66.7/241.4 MB 6.2 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 66.8/241.4 MB 6.1 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 66.8/241.4 MB 5.9 MB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 66.8/241.4 MB 5.8 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 66.8/241.4 MB 5.8 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 66.8/241.4 MB 5.8 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 66.9/241.4 MB 5.4 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 67.3/241.4 MB 5.5 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 67.5/241.4 MB 5.5 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 68.1/241.4 MB 5.4 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 68.5/241.4 MB 5.4 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 68.9/241.4 MB 5.3 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 69.5/241.4 MB 5.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 70.1/241.4 MB 5.7 MB/s eta 0:00:30\n",
      "   ----------- ---------------------------- 70.5/241.4 MB 5.7 MB/s eta 0:00:31\n",
      "   ----------- ---------------------------- 70.9/241.4 MB 6.0 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 71.6/241.4 MB 6.0 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 71.9/241.4 MB 5.9 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 72.3/241.4 MB 5.8 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 72.8/241.4 MB 5.9 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 73.2/241.4 MB 5.8 MB/s eta 0:00:29\n",
      "   ------------ --------------------------- 73.8/241.4 MB 6.0 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 74.2/241.4 MB 6.1 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 74.8/241.4 MB 6.1 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 75.2/241.4 MB 6.2 MB/s eta 0:00:27\n",
      "   ------------ --------------------------- 75.5/241.4 MB 6.1 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 76.2/241.4 MB 6.2 MB/s eta 0:00:27\n",
      "   ------------ --------------------------- 76.6/241.4 MB 6.8 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 76.9/241.4 MB 7.5 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 77.5/241.4 MB 9.4 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 78.1/241.4 MB 9.8 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 78.5/241.4 MB 9.6 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 78.7/241.4 MB 9.6 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 79.4/241.4 MB 9.6 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 79.8/241.4 MB 9.6 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 80.3/241.4 MB 9.5 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 80.4/241.4 MB 9.2 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 80.8/241.4 MB 9.2 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 81.1/241.4 MB 9.2 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 81.6/241.4 MB 9.1 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 82.0/241.4 MB 9.1 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 82.4/241.4 MB 9.0 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 82.8/241.4 MB 9.1 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 83.2/241.4 MB 9.1 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 83.5/241.4 MB 9.2 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 83.9/241.4 MB 8.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 84.2/241.4 MB 8.8 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 84.4/241.4 MB 8.8 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 84.7/241.4 MB 8.5 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 85.1/241.4 MB 8.6 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 85.5/241.4 MB 8.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 86.0/241.4 MB 8.6 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 86.2/241.4 MB 8.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 86.7/241.4 MB 8.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 87.0/241.4 MB 8.3 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 87.4/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 88.1/241.4 MB 8.3 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 88.3/241.4 MB 8.1 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 88.8/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 89.0/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 89.6/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 89.9/241.4 MB 8.1 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 90.3/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 90.7/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 91.0/241.4 MB 8.3 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 91.2/241.4 MB 7.9 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 91.7/241.4 MB 8.1 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 92.0/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 92.7/241.4 MB 8.3 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 93.1/241.4 MB 8.2 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 93.5/241.4 MB 8.1 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 93.6/241.4 MB 8.0 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 93.9/241.4 MB 8.1 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 94.5/241.4 MB 8.2 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 95.0/241.4 MB 8.4 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 95.5/241.4 MB 8.6 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 95.8/241.4 MB 8.3 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 96.2/241.4 MB 8.5 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 96.7/241.4 MB 8.4 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 97.0/241.4 MB 8.4 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 97.1/241.4 MB 8.2 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 97.6/241.4 MB 8.1 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 98.0/241.4 MB 8.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 98.5/241.4 MB 8.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 98.9/241.4 MB 8.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 99.3/241.4 MB 8.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 99.6/241.4 MB 8.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 100.0/241.4 MB 8.2 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 100.5/241.4 MB 8.1 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 100.6/241.4 MB 8.3 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 101.2/241.4 MB 8.2 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 101.6/241.4 MB 8.5 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 101.8/241.4 MB 8.3 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 102.3/241.4 MB 8.3 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 102.5/241.4 MB 8.1 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 102.9/241.4 MB 8.0 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 102.9/241.4 MB 8.0 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 103.0/241.4 MB 7.5 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 103.6/241.4 MB 7.6 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 104.0/241.4 MB 7.9 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 104.6/241.4 MB 8.0 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 105.2/241.4 MB 7.9 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 105.4/241.4 MB 7.9 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 105.5/241.4 MB 7.5 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 106.0/241.4 MB 7.6 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 106.3/241.4 MB 7.7 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 106.5/241.4 MB 7.4 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 107.0/241.4 MB 7.5 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 107.0/241.4 MB 7.4 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 107.4/241.4 MB 7.4 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 107.9/241.4 MB 7.5 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 108.5/241.4 MB 7.6 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 108.8/241.4 MB 7.5 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 109.2/241.4 MB 7.6 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 109.5/241.4 MB 7.5 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 109.5/241.4 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 109.7/241.4 MB 7.2 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 110.4/241.4 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 110.9/241.4 MB 7.5 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 111.4/241.4 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 111.7/241.4 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 112.0/241.4 MB 7.6 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 112.1/241.4 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 112.6/241.4 MB 7.3 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 112.8/241.4 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 113.5/241.4 MB 8.0 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 113.8/241.4 MB 7.9 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 114.2/241.4 MB 7.9 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 114.4/241.4 MB 7.6 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 114.7/241.4 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 115.4/241.4 MB 7.6 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 115.7/241.4 MB 7.9 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 116.2/241.4 MB 7.9 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 116.5/241.4 MB 7.8 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 116.8/241.4 MB 7.9 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 117.1/241.4 MB 7.8 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 117.7/241.4 MB 8.2 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 118.1/241.4 MB 8.3 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 118.6/241.4 MB 8.1 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 118.7/241.4 MB 7.9 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 119.2/241.4 MB 7.9 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 119.5/241.4 MB 7.9 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 120.1/241.4 MB 8.4 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 120.7/241.4 MB 8.4 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 120.9/241.4 MB 8.3 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 121.4/241.4 MB 8.2 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 121.7/241.4 MB 8.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 121.7/241.4 MB 8.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 122.0/241.4 MB 7.9 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 122.5/241.4 MB 8.3 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 123.0/241.4 MB 8.3 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 123.5/241.4 MB 8.2 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 123.8/241.4 MB 8.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 124.2/241.4 MB 8.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 124.4/241.4 MB 8.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 124.9/241.4 MB 8.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 125.3/241.4 MB 8.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 125.7/241.4 MB 8.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 126.2/241.4 MB 8.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 126.5/241.4 MB 8.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 126.7/241.4 MB 7.8 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 127.3/241.4 MB 8.1 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 127.5/241.4 MB 8.2 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 127.8/241.4 MB 7.8 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 128.1/241.4 MB 7.6 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 128.5/241.4 MB 7.6 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 129.0/241.4 MB 7.8 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 129.5/241.4 MB 7.8 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 129.7/241.4 MB 7.8 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 130.3/241.4 MB 7.7 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 130.6/241.4 MB 7.7 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 130.9/241.4 MB 7.4 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 131.1/241.4 MB 7.6 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 131.5/241.4 MB 7.4 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 132.1/241.4 MB 7.8 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 132.5/241.4 MB 7.7 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 132.9/241.4 MB 7.9 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 133.4/241.4 MB 7.8 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 133.6/241.4 MB 7.7 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 133.9/241.4 MB 7.6 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 133.9/241.4 MB 7.6 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 134.5/241.4 MB 7.5 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 135.1/241.4 MB 7.7 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 135.6/241.4 MB 7.8 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 135.8/241.4 MB 7.5 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 136.1/241.4 MB 7.6 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 136.3/241.4 MB 7.5 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 136.5/241.4 MB 7.3 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 137.0/241.4 MB 7.5 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 137.5/241.4 MB 7.5 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 137.8/241.4 MB 7.8 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 138.2/241.4 MB 7.7 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 138.2/241.4 MB 7.7 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 138.5/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 139.0/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 139.5/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 139.8/241.4 MB 7.3 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 140.1/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 140.1/241.4 MB 7.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 140.3/241.4 MB 7.0 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 140.9/241.4 MB 7.2 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 141.4/241.4 MB 7.3 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 141.8/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 141.8/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 141.8/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 141.8/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 141.8/241.4 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 141.9/241.4 MB 6.3 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 141.9/241.4 MB 6.1 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.0/241.4 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.0/241.4 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.0/241.4 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.0/241.4 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.0/241.4 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.0/241.4 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.0/241.4 MB 6.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 142.1/241.4 MB 5.0 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 142.2/241.4 MB 4.9 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 142.4/241.4 MB 4.9 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 142.4/241.4 MB 4.7 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 142.6/241.4 MB 4.7 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 142.7/241.4 MB 4.6 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 143.1/241.4 MB 4.6 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 143.5/241.4 MB 4.5 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 143.9/241.4 MB 4.6 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 144.3/241.4 MB 4.8 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 144.4/241.4 MB 4.7 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 144.7/241.4 MB 4.6 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 145.2/241.4 MB 4.5 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 145.6/241.4 MB 4.5 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 146.0/241.4 MB 4.6 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 146.0/241.4 MB 4.5 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 146.3/241.4 MB 4.5 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 146.7/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 147.4/241.4 MB 4.6 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 147.7/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 147.9/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 148.1/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 148.4/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 148.8/241.4 MB 4.6 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 149.1/241.4 MB 4.6 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 149.6/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 149.7/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 149.9/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 150.3/241.4 MB 4.5 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 150.8/241.4 MB 4.7 MB/s eta 0:00:20\n",
      "   ------------------------- -------------- 151.3/241.4 MB 4.7 MB/s eta 0:00:20\n",
      "   ------------------------- -------------- 151.6/241.4 MB 4.6 MB/s eta 0:00:20\n",
      "   ------------------------- -------------- 151.8/241.4 MB 4.5 MB/s eta 0:00:20\n",
      "   ------------------------- -------------- 151.9/241.4 MB 4.4 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 152.4/241.4 MB 6.3 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 152.9/241.4 MB 6.8 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 153.4/241.4 MB 6.9 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 153.7/241.4 MB 6.9 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 154.0/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 154.1/241.4 MB 6.6 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 154.6/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 155.0/241.4 MB 6.9 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 155.4/241.4 MB 6.9 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 155.7/241.4 MB 6.8 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 155.7/241.4 MB 6.8 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 156.0/241.4 MB 6.5 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 156.4/241.4 MB 7.0 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 157.0/241.4 MB 6.9 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 157.2/241.4 MB 6.8 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 157.5/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 157.6/241.4 MB 6.4 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 158.1/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 158.5/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 158.9/241.4 MB 6.9 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 159.3/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 159.5/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 159.6/241.4 MB 6.6 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 159.9/241.4 MB 6.4 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 160.3/241.4 MB 6.8 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 160.8/241.4 MB 6.7 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 161.3/241.4 MB 6.7 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 161.3/241.4 MB 6.7 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 161.4/241.4 MB 6.3 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 161.9/241.4 MB 6.5 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 162.1/241.4 MB 6.6 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 162.7/241.4 MB 6.7 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 162.8/241.4 MB 6.8 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 163.1/241.4 MB 6.4 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 163.4/241.4 MB 6.4 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 163.5/241.4 MB 6.2 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 163.8/241.4 MB 6.2 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 164.1/241.4 MB 6.3 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 164.6/241.4 MB 6.4 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 165.0/241.4 MB 6.5 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 165.0/241.4 MB 6.2 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 165.1/241.4 MB 6.1 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 165.6/241.4 MB 6.1 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 166.0/241.4 MB 6.4 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 166.6/241.4 MB 6.4 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 166.7/241.4 MB 6.5 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 166.7/241.4 MB 6.5 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 167.1/241.4 MB 6.1 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 167.7/241.4 MB 6.3 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 168.0/241.4 MB 6.4 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 168.6/241.4 MB 6.5 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 168.6/241.4 MB 6.5 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 168.8/241.4 MB 6.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 169.1/241.4 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 169.5/241.4 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 169.8/241.4 MB 6.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 170.1/241.4 MB 6.4 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 170.7/241.4 MB 6.5 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 170.7/241.4 MB 6.5 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 170.7/241.4 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 171.1/241.4 MB 6.0 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 171.5/241.4 MB 6.0 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 172.0/241.4 MB 6.3 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 172.2/241.4 MB 6.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 172.4/241.4 MB 6.2 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 172.7/241.4 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 172.8/241.4 MB 6.0 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 173.1/241.4 MB 6.0 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 173.3/241.4 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 173.5/241.4 MB 5.9 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 174.0/241.4 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 174.0/241.4 MB 6.1 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 174.1/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 174.4/241.4 MB 5.7 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 174.9/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 175.3/241.4 MB 6.1 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 175.7/241.4 MB 6.1 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 175.7/241.4 MB 6.1 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 175.7/241.4 MB 5.8 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 176.0/241.4 MB 5.7 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 176.4/241.4 MB 5.7 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 177.0/241.4 MB 6.0 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 177.5/241.4 MB 6.0 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 177.6/241.4 MB 5.9 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 177.8/241.4 MB 5.7 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 177.9/241.4 MB 5.5 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 178.3/241.4 MB 5.6 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 178.8/241.4 MB 5.8 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 179.0/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 179.5/241.4 MB 5.9 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 179.6/241.4 MB 5.8 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 179.7/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 179.7/241.4 MB 5.6 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 180.0/241.4 MB 5.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 180.7/241.4 MB 5.6 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 181.0/241.4 MB 5.9 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 181.2/241.4 MB 5.9 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 181.5/241.4 MB 5.8 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 181.7/241.4 MB 5.7 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 181.8/241.4 MB 5.6 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 182.2/241.4 MB 5.6 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 182.5/241.4 MB 5.6 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 183.2/241.4 MB 6.0 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 183.5/241.4 MB 6.0 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 183.5/241.4 MB 6.0 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 183.6/241.4 MB 5.8 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 183.8/241.4 MB 5.8 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 184.3/241.4 MB 6.1 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 184.5/241.4 MB 6.1 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 184.8/241.4 MB 6.1 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 185.1/241.4 MB 6.0 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 185.3/241.4 MB 5.8 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 185.3/241.4 MB 5.8 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 185.3/241.4 MB 5.6 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 186.2/241.4 MB 6.2 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 186.5/241.4 MB 6.1 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 186.6/241.4 MB 6.0 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 186.7/241.4 MB 5.9 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 187.0/241.4 MB 5.8 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 187.3/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 187.7/241.4 MB 5.6 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 187.9/241.4 MB 5.8 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 188.0/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 188.3/241.4 MB 5.8 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 188.7/241.4 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 188.9/241.4 MB 5.6 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 189.3/241.4 MB 5.6 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 189.5/241.4 MB 5.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 189.6/241.4 MB 5.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 190.0/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 190.3/241.4 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 190.6/241.4 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 191.0/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 191.1/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 191.2/241.4 MB 5.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 191.4/241.4 MB 5.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 191.7/241.4 MB 5.5 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 192.2/241.4 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 192.5/241.4 MB 5.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 192.7/241.4 MB 5.5 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 192.8/241.4 MB 5.5 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 193.1/241.4 MB 5.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 193.4/241.4 MB 5.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 193.7/241.4 MB 5.2 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 194.2/241.4 MB 5.6 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 194.2/241.4 MB 5.6 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 194.2/241.4 MB 5.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 194.5/241.4 MB 5.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 194.9/241.4 MB 5.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 195.4/241.4 MB 5.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 195.7/241.4 MB 5.7 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 195.7/241.4 MB 5.7 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 195.7/241.4 MB 5.7 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 195.9/241.4 MB 5.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 196.3/241.4 MB 5.2 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 196.9/241.4 MB 5.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.2/241.4 MB 5.4 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.4/241.4 MB 5.5 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.4/241.4 MB 5.5 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.4/241.4 MB 5.5 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.4/241.4 MB 5.1 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.5/241.4 MB 5.0 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 197.5/241.4 MB 4.9 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.7/241.4 MB 4.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.8/241.4 MB 4.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.8/241.4 MB 4.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.9/241.4 MB 4.5 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.9/241.4 MB 4.5 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.9/241.4 MB 4.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.9/241.4 MB 4.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.9/241.4 MB 4.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 198.0/241.4 MB 4.1 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 198.2/241.4 MB 4.1 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 198.4/241.4 MB 4.1 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 198.8/241.4 MB 4.2 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 199.1/241.4 MB 4.2 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 199.6/241.4 MB 4.3 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 200.1/241.4 MB 4.4 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 200.4/241.4 MB 4.3 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 200.9/241.4 MB 4.4 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 201.2/241.4 MB 4.4 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 201.6/241.4 MB 4.6 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 201.9/241.4 MB 4.5 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 202.4/241.4 MB 4.6 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 202.7/241.4 MB 4.6 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 203.0/241.4 MB 4.6 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 203.4/241.4 MB 4.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 203.8/241.4 MB 4.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 204.2/241.4 MB 4.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 204.5/241.4 MB 5.1 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 204.8/241.4 MB 5.0 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 205.1/241.4 MB 5.0 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 205.6/241.4 MB 5.0 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 206.1/241.4 MB 5.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 206.5/241.4 MB 5.3 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 206.8/241.4 MB 5.3 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 207.2/241.4 MB 5.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 207.6/241.4 MB 5.4 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 207.9/241.4 MB 6.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 208.4/241.4 MB 8.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 208.8/241.4 MB 8.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 209.1/241.4 MB 8.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 209.5/241.4 MB 8.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 209.9/241.4 MB 8.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 210.3/241.4 MB 8.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 210.5/241.4 MB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 210.8/241.4 MB 7.9 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 211.5/241.4 MB 8.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 211.8/241.4 MB 8.0 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 212.1/241.4 MB 8.0 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 212.3/241.4 MB 7.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 212.8/241.4 MB 7.8 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 213.2/241.4 MB 7.8 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 213.6/241.4 MB 7.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 213.8/241.4 MB 7.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 214.0/241.4 MB 7.6 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 214.3/241.4 MB 7.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 214.8/241.4 MB 7.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 215.1/241.4 MB 7.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 215.3/241.4 MB 7.6 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 215.5/241.4 MB 7.4 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 216.0/241.4 MB 7.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 216.4/241.4 MB 7.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 216.7/241.4 MB 7.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 217.0/241.4 MB 7.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 217.2/241.4 MB 7.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 217.6/241.4 MB 7.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 217.8/241.4 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 218.1/241.4 MB 7.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 218.5/241.4 MB 7.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 218.5/241.4 MB 6.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 219.0/241.4 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 219.2/241.4 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 219.6/241.4 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 219.9/241.4 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 220.2/241.4 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 220.3/241.4 MB 6.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 220.8/241.4 MB 6.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 221.1/241.4 MB 6.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 221.6/241.4 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 221.9/241.4 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 222.0/241.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 222.3/241.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 222.6/241.4 MB 6.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 223.0/241.4 MB 6.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 223.3/241.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 223.5/241.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 223.7/241.4 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 224.2/241.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 224.5/241.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 224.8/241.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 224.9/241.4 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 225.2/241.4 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 225.2/241.4 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 225.6/241.4 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 226.1/241.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 226.5/241.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 226.9/241.4 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 227.0/241.4 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 227.2/241.4 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 227.6/241.4 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 228.1/241.4 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 228.3/241.4 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 228.7/241.4 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 228.8/241.4 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 229.2/241.4 MB 6.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 229.3/241.4 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 229.9/241.4 MB 6.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 229.9/241.4 MB 6.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 230.3/241.4 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 230.6/241.4 MB 6.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 230.8/241.4 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 231.2/241.4 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 231.5/241.4 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 231.8/241.4 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 232.0/241.4 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 232.3/241.4 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 232.4/241.4 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 232.8/241.4 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 233.2/241.4 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 233.5/241.4 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 233.7/241.4 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 233.7/241.4 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 234.1/241.4 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 234.6/241.4 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 235.0/241.4 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 235.3/241.4 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  235.5/241.4 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  235.5/241.4 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  235.7/241.4 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  236.1/241.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  236.5/241.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  236.9/241.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  237.2/241.4 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  237.2/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  237.6/241.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  237.9/241.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  238.3/241.4 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  238.5/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  238.7/241.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  238.8/241.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  239.0/241.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  239.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  239.8/241.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  240.0/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  240.1/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  240.3/241.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  240.3/241.4 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  240.8/241.4 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.0/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.3/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.4/241.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 241.4/241.4 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, accelerate\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1\n",
      "    Uninstalling torch-2.7.1:\n",
      "      Successfully uninstalled torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentencepiece accelerate torch --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99205057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/nllb-200-distilled-600M\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# ===== INPUTS YOU ALREADY HAVE =====\n",
    "EVAL_SU_ID = Path(\"out_eval_ready/eval_test_su_id.csv\")     # has sent_id, src (Sundanese), ref (Indonesian), split\n",
    "EVAL_SU_EN = Path(\"out_eval_ready/eval_test_su_en.csv\")     # has sent_id, src (Sundanese), ref (English), split\n",
    "TPL_SU_ID  = Path(\"out_eval_ready/test_template_su_id.tsv\") # has sent_id\n",
    "TPL_SU_EN  = Path(\"out_eval_ready/test_template_su_en.tsv\") # has sent_id\n",
    "\n",
    "# ===== OUTPUTS (for SYSTEMS) =====\n",
    "OUT_DIR = Path(\"./runs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_TSV_SU_ID = OUT_DIR / \"nllb_su_id_test.tsv\"\n",
    "OUT_TSV_SU_EN = OUT_DIR / \"nllb_su_en_test.tsv\"\n",
    "\n",
    "# ===== Translator setup (NLLB-200 distilled) =====\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# smaller/faster than full 3.3B:\n",
    "\n",
    "# ===== NLLB setup (fixed) =====\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "print(\"Loading model:\", MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# NLLB language codes\n",
    "SRC = \"sun_Latn\"   # Sundanese (Latin)\n",
    "TGT_ID = \"ind_Latn\"\n",
    "TGT_EN = \"eng_Latn\"\n",
    "\n",
    "# IMPORTANT: set source lang *on the tokenizer* (don't pass as kwarg to __call__)\n",
    "tokenizer.src_lang = SRC\n",
    "\n",
    "def _bos_id(tgt_code: str) -> int:\n",
    "    # Newer tokenizer versions have lang_code_to_id\n",
    "    if hasattr(tokenizer, \"lang_code_to_id\") and tgt_code in tokenizer.lang_code_to_id:\n",
    "        return tokenizer.lang_code_to_id[tgt_code]\n",
    "    # Fallback (older behavior)\n",
    "    return tokenizer.convert_tokens_to_ids(tgt_code)\n",
    "\n",
    "def translate_batch(texts, tgt_code: str, max_new_tokens=128, batch_size=32, num_beams=4):\n",
    "    out = []\n",
    "    bos_id = _bos_id(tgt_code)\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = [t if isinstance(t, str) else \"\" for t in texts[i:i+batch_size]]\n",
    "        enc = tokenizer(\n",
    "            chunk,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            forced_bos_token_id=bos_id,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams\n",
    "        )\n",
    "        out.extend(tokenizer.batch_decode(gen, skip_special_tokens=True))\n",
    "    return out\n",
    "\n",
    "def build_system_tsv(eval_csv: Path, tpl_tsv: Path, tgt_code: str, out_tsv: Path):\n",
    "    # Join template with eval to recover source sentences in the right order\n",
    "    tpl = pd.read_csv(tpl_tsv, sep=\"\\t\", dtype=str)\n",
    "    eva = pd.read_csv(eval_csv, dtype=str)\n",
    "    assert {\"sent_id\",\"src\"}.issubset(eva.columns), f\"{eval_csv} must have sent_id, src\"\n",
    "    df = tpl.merge(eva[[\"sent_id\",\"src\"]], on=\"sent_id\", how=\"left\")\n",
    "    assert df[\"src\"].notna().all(), \"Some sent_id from template not found in eval CSV.\"\n",
    "\n",
    "    hyps = translate_batch(df[\"src\"].tolist(), tgt_code=tgt_code, max_new_tokens=128, batch_size=32)\n",
    "    df[\"hyp\"] = hyps\n",
    "    df[[\"sent_id\",\"hyp\"]].to_csv(out_tsv, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "    print(\"Wrote →\", out_tsv)\n",
    "\n",
    "# Build both pairs\n",
    "build_system_tsv(EVAL_SU_ID, TPL_SU_ID, TGT_ID, OUT_TSV_SU_ID)  # su→id\n",
    "build_system_tsv(EVAL_SU_EN, TPL_SU_EN, TGT_EN, OUT_TSV_SU_EN)  # su→en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13537f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de138a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a9f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc24bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ecd081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aebf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17620a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bcc65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeae3400589b42d5bfe63f1c32cb4854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jtkacer01\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using COMET: Unbabel/wmt22-comet-da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtkacer01\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pair</th>\n",
       "      <th>System</th>\n",
       "      <th>Unfiltered BLEU</th>\n",
       "      <th>Filtered BLEU</th>\n",
       "      <th>Weighted BLEU</th>\n",
       "      <th>Unfiltered ChrF</th>\n",
       "      <th>Filtered ChrF</th>\n",
       "      <th>Weighted ChrF</th>\n",
       "      <th>Unfiltered COMET</th>\n",
       "      <th>Filtered COMET</th>\n",
       "      <th>Weighted COMET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Pair, System, Unfiltered BLEU, Filtered BLEU, Weighted BLEU, Unfiltered ChrF, Filtered ChrF, Weighted ChrF, Unfiltered COMET, Filtered COMET, Weighted COMET]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results table → out_eval\\mt_results_table.csv\n"
     ]
    }
   ],
   "source": [
    "# MT results table with COMET→BERTScore fallback.\n",
    "# Inputs:\n",
    "#   - TEST_FLAGGED_CSV: CSV with columns [sent_id, src, ref, flag, weight, split] (use your eval_test_* files)\n",
    "#   - SYSTEMS: list of dicts {\"pair\": \"su-id\"/\"su-en\", \"system\": \"...\", \"hyp_tsv\": \"...\"} (sent_id, hyp)\n",
    "\n",
    "import csv, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# ======== CONFIG ========\n",
    "# Point this to the eval file you generated for EACH pair before running for that pair:\n",
    "TEST_FLAGGED_CSV = Path(\"./out_eval_ready/eval_test_su_id.csv\")  # or eval_test_su_en.csv\n",
    "USE_SPLIT = True  # use only split == test\n",
    "\n",
    "SYSTEMS = [\n",
    "    # Fill with your systems (examples below); you can run this cell multiple times for different sets/pairs.\n",
    "    # {\"pair\": \"su-id\", \"system\": \"Transformer-base\", \"hyp_tsv\": \"./runs/transformer_su_id_test.tsv\"},\n",
    "    # {\"pair\": \"su-id\", \"system\": \"Moses-SMT\",        \"hyp_tsv\": \"./runs/moses_su_id_test.tsv\"},\n",
    "    # {\"pair\": \"su-en\", \"system\": \"mBART50\",          \"hyp_tsv\": \"./runs/mbart50_su_en_test.tsv\"},\n",
    "]\n",
    "\n",
    "EXPECTED_TEST_SIZE = 1000   # informational print only\n",
    "\n",
    "# ======== Metric backends ========\n",
    "# BLEU/ChrF (sacrebleu)\n",
    "try:\n",
    "    import sacrebleu\n",
    "except Exception as e:\n",
    "    sacrebleu = None\n",
    "    print(\"[WARN] sacrebleu not available. Install with: pip install sacrebleu\")\n",
    "\n",
    "# Try COMET first\n",
    "COMET_AVAILABLE = False\n",
    "try:\n",
    "    from comet import download_model, load_from_checkpoint\n",
    "    _ckpt = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    comet_model = load_from_checkpoint(_ckpt)\n",
    "    COMET_AVAILABLE = True\n",
    "    print(\"[INFO] Using COMET: Unbabel/wmt22-comet-da\")\n",
    "except Exception:\n",
    "    comet_model = None\n",
    "    print(\"[INFO] COMET not available; will try BERTScore.\")\n",
    "\n",
    "# Prepare BERTScore fallback (lazy import inside function)\n",
    "_BERT_OK = None\n",
    "\n",
    "def _ensure_bertscore():\n",
    "    global _BERT_OK, bert_score_score\n",
    "    if _BERT_OK is not None:\n",
    "        return _BERT_OK\n",
    "    try:\n",
    "        from bert_score import score as bert_score_score\n",
    "        globals()[\"bert_score_score\"] = bert_score_score\n",
    "        _BERT_OK = True\n",
    "    except Exception:\n",
    "        _BERT_OK = False\n",
    "    return _BERT_OK\n",
    "\n",
    "# ======== Helpers ========\n",
    "def load_flagged_test_rows(path: Path, pair: str, use_split: bool) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    if \"weight\" in df.columns:\n",
    "        df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\").fillna(0.0)\n",
    "    else:\n",
    "        df[\"weight\"] = 0.0\n",
    "    if \"flag\" not in df.columns:\n",
    "        df[\"flag\"] = \"Medium\"\n",
    "\n",
    "    # select target column based on pair\n",
    "    if pair == \"su-id\":\n",
    "        # in eval_test_* we already named columns \"src\",\"ref\"\n",
    "        pass\n",
    "    elif pair == \"su-en\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pair: {pair}\")\n",
    "\n",
    "    need = {\"sent_id\",\"src\",\"ref\",\"flag\",\"weight\"}\n",
    "    miss = [c for c in need if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing columns in {path}: {miss}\")\n",
    "\n",
    "    if use_split and \"split\" in df.columns:\n",
    "        df = df[df[\"split\"].astype(str).str.lower().eq(\"test\")].copy()\n",
    "\n",
    "    df = df[df[\"ref\"].astype(str).str.strip().ne(\"\")].copy()\n",
    "    df[\"sent_id\"] = df[\"sent_id\"].astype(str).str.strip()\n",
    "    df[\"src\"] = df[\"src\"].astype(str).str.strip()\n",
    "    df[\"ref\"] = df[\"ref\"].astype(str).str.strip()\n",
    "    if EXPECTED_TEST_SIZE:\n",
    "        print(f\"[{pair}] Loaded {len(df)} test rows (expected ~{EXPECTED_TEST_SIZE}).\")\n",
    "    return df[[\"sent_id\",\"src\",\"ref\",\"flag\",\"weight\"]].copy()\n",
    "\n",
    "def load_hyp_tsv(path: Path) -> pd.DataFrame:\n",
    "    # support TSV (preferred) or CSV\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "    assert \"sent_id\" in df.columns and \"hyp\" in df.columns, f\"{path} must have columns: sent_id, hyp\"\n",
    "    df[\"sent_id\"] = df[\"sent_id\"].astype(str).str.strip()\n",
    "    df[\"hyp\"] = df[\"hyp\"].astype(str).str.strip()\n",
    "    return df[[\"sent_id\",\"hyp\"]].copy()\n",
    "\n",
    "def join_test_hyp(test_df: pd.DataFrame, hyp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    merged = test_df.merge(hyp_df, on=\"sent_id\", how=\"inner\")\n",
    "    dropped = len(test_df) - len(merged)\n",
    "    if dropped:\n",
    "        print(f\"[WARN] {dropped} test rows had no hypothesis and were dropped.\")\n",
    "    return merged\n",
    "\n",
    "def compute_bleu_chrf_lists(refs: List[str], hyps: List[str]) -> Tuple[float, float, List[float], List[float]]:\n",
    "    if sacrebleu is None:\n",
    "        return float(\"nan\"), float(\"nan\"), [], []\n",
    "    bleu_c = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "    chrf_c = sacrebleu.corpus_chrf(hyps, [refs]).score\n",
    "    bleu_s, chrf_s = [], []\n",
    "    for h, r in zip(hyps, refs):\n",
    "        bleu_s.append(sacrebleu.sentence_bleu(h, [r]).score)\n",
    "        chrf_s.append(sacrebleu.sentence_chrf(h, [r]).score)\n",
    "    return bleu_c, chrf_c, bleu_s, chrf_s\n",
    "\n",
    "def weighted_mean(scores: List[float], weights: List[float]) -> float:\n",
    "    num = sum(s*w for s, w in zip(scores, weights))\n",
    "    den = sum(weights)\n",
    "    return (num/den) if den > 0 else float(\"nan\")\n",
    "\n",
    "def tgt_lang_for_pair(pair: str) -> str:\n",
    "    # Language code for BERTScore; adjust if you use a different tag set\n",
    "    return \"id\" if pair == \"su-id\" else \"en\"\n",
    "\n",
    "def compute_comet_or_bertscore(pair: str, srcs: List[str], hyps: List[str], refs: List[str]) -> Tuple[float, List[float]]:\n",
    "    # Try COMET first\n",
    "    if COMET_AVAILABLE and comet_model is not None:\n",
    "        data = [{\"src\": s, \"mt\": h, \"ref\": r} for s,h,r in zip(srcs, hyps, refs)]\n",
    "        out = comet_model.predict(data, batch_size=32, gpus=0)\n",
    "        sent = out[\"scores\"]\n",
    "        corpus = float(sum(sent) / max(1, len(sent)))\n",
    "        return corpus, sent\n",
    "\n",
    "    # Fallback: BERTScore F1\n",
    "    if _ensure_bertscore():\n",
    "        lang = tgt_lang_for_pair(pair)\n",
    "        P, R, F1 = bert_score_score(hyps, refs, lang=lang, verbose=False)\n",
    "        sent = [float(x) for x in F1.tolist()]\n",
    "        corpus = float(sum(sent) / max(1, len(sent)))\n",
    "        return corpus, sent\n",
    "\n",
    "    # Final fallback\n",
    "    return float(\"nan\"), []\n",
    "\n",
    "def evaluate_split(pair: str, df: pd.DataFrame, use_filter: bool, use_weighted: bool) -> Dict[str, float]:\n",
    "    if use_filter:\n",
    "        df = df[df[\"flag\"].astype(str).str.lower().eq(\"high\")].copy()\n",
    "    if len(df) == 0:\n",
    "        return {\"BLEU\": float(\"nan\"), \"ChrF\": float(\"nan\"), \"COMET\": float(\"nan\")}\n",
    "\n",
    "    refs = df[\"ref\"].tolist()\n",
    "    hyps = df[\"hyp\"].tolist()\n",
    "    srcs = df[\"src\"].tolist()\n",
    "    weights = df[\"weight\"].astype(float).tolist()\n",
    "\n",
    "    bleu_c, chrf_c, bleu_s, chrf_s = compute_bleu_chrf_lists(refs, hyps)\n",
    "    comet_c, comet_s = compute_comet_or_bertscore(pair, srcs, hyps, refs)\n",
    "\n",
    "    if use_weighted:\n",
    "        bleu = weighted_mean(bleu_s, weights)\n",
    "        chrf = weighted_mean(chrf_s, weights)\n",
    "        comet = weighted_mean(comet_s, weights) if comet_s else float(\"nan\")\n",
    "    else:\n",
    "        bleu, chrf, comet = bleu_c, chrf_c, comet_c\n",
    "\n",
    "    return {\"BLEU\": bleu, \"ChrF\": chrf, \"COMET\": comet}\n",
    "\n",
    "# ======== Build results table ========\n",
    "rows_out = []\n",
    "\n",
    "for spec in SYSTEMS:\n",
    "    pair = spec[\"pair\"].strip().lower()\n",
    "    system_name = spec[\"system\"]\n",
    "    hyp_path = Path(spec[\"hyp_tsv\"])\n",
    "\n",
    "    test_df = load_flagged_test_rows(TEST_FLAGGED_CSV, pair, USE_SPLIT)\n",
    "    hyp_df = load_hyp_tsv(hyp_path)\n",
    "    merged = join_test_hyp(test_df, hyp_df)\n",
    "\n",
    "    unfiltered = evaluate_split(pair, merged, use_filter=False, use_weighted=False)\n",
    "    filtered   = evaluate_split(pair, merged, use_filter=True,  use_weighted=False)\n",
    "    weighted   = evaluate_split(pair, merged, use_filter=False, use_weighted=True)\n",
    "\n",
    "    rows_out.append({\n",
    "        \"Pair\": pair,\n",
    "        \"System\": system_name,\n",
    "        \"Unfiltered BLEU\": round(unfiltered[\"BLEU\"], 2) if pd.notna(unfiltered[\"BLEU\"]) else float(\"nan\"),\n",
    "        \"Filtered BLEU\":   round(filtered[\"BLEU\"],   2) if pd.notna(filtered[\"BLEU\"])   else float(\"nan\"),\n",
    "        \"Weighted BLEU\":   round(weighted[\"BLEU\"],   2) if pd.notna(weighted[\"BLEU\"])   else float(\"nan\"),\n",
    "        \"Unfiltered ChrF\": round(unfiltered[\"ChrF\"], 2) if pd.notna(unfiltered[\"ChrF\"]) else float(\"nan\"),\n",
    "        \"Filtered ChrF\":   round(filtered[\"ChrF\"],   2) if pd.notna(filtered[\"ChrF\"])   else float(\"nan\"),\n",
    "        \"Weighted ChrF\":   round(weighted[\"ChrF\"],   2) if pd.notna(weighted[\"ChrF\"])   else float(\"nan\"),\n",
    "        # These columns are COMET if available, else BERTScore F1\n",
    "        \"Unfiltered COMET\":round(unfiltered[\"COMET\"],3) if pd.notna(unfiltered[\"COMET\"])else float(\"nan\"),\n",
    "        \"Filtered COMET\":  round(filtered[\"COMET\"],  3) if pd.notna(filtered[\"COMET\"])  else float(\"nan\"),\n",
    "        \"Weighted COMET\":  round(weighted[\"COMET\"],  3) if pd.notna(weighted[\"COMET\"])  else float(\"nan\"),\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows_out, columns=[\n",
    "    \"Pair\",\"System\",\n",
    "    \"Unfiltered BLEU\",\"Filtered BLEU\",\"Weighted BLEU\",\n",
    "    \"Unfiltered ChrF\",\"Filtered ChrF\",\"Weighted ChrF\",\n",
    "    \"Unfiltered COMET\",\"Filtered COMET\",\"Weighted COMET\",\n",
    "])\n",
    "\n",
    "save_path = Path(\"./out_eval/mt_results_table.csv\")\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
    "display(df_out)\n",
    "print(f\"\\nSaved results table → {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c5551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
